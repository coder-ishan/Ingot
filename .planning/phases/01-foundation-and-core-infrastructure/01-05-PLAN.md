---
phase: 01-foundation-and-core-infrastructure
plan: 05
type: execute
wave: 4
depends_on:
  - "01-01"
  - "01-02"
  - "01-03"
  - "01-04"
files_modified:
  - tests/__init__.py
  - tests/conftest.py
  - tests/fixtures/yc_companies.json
  - tests/fixtures/user_profile.json
  - tests/fixtures/intel_brief.json
  - tests/unit/__init__.py
  - tests/unit/test_crypto.py
  - tests/unit/test_config.py
  - tests/unit/test_llm_client.py
  - tests/unit/test_pydantic_validation.py
  - tests/unit/test_retry.py
  - tests/unit/test_db_models.py
  - tests/unit/test_dispatcher.py
  - tests/unit/test_http_client.py
  - tests/unit/test_agent_framework.py
  - tests/unit/test_agent_imports.py
  - tests/unit/test_import_boundaries.py
  - tests/unit/test_exceptions.py
  - tests/unit/test_performance.py
  - tests/integration/__init__.py
  - tests/integration/test_setup_wizard.py
  - tests/integration/test_db_wal.py
  - tests/integration/test_alembic_migration.py
autonomous: true
requirements:
  - TEST-P1-01
  - TEST-P1-02
  - TEST-P1-03
  - TEST-P1-04
  - TEST-P1-05
  - TEST-P1-06
  - TEST-P1-07
  - TEST-P1-08
  - TEST-P1-09
  - TEST-INFRA-01
  - TEST-INFRA-02
  - TEST-INFRA-03
  - TEST-INFRA-04
  - TEST-INFRA-05
  - TEST-INFRA-06
  - TEST-INFRA-07
  - TEST-INFRA-08

must_haves:
  truths:
    - "pytest tests/ -x -q runs to completion without errors in under 30 seconds"
    - "pytest --cov=ingot --cov-fail-under=70 passes — 70% overall coverage, 80%+ on crypto/DB/LLMClient"
    - "Zero real API calls are made during the test suite — all LLM calls use PydanticAI TestModel or AsyncMock"
    - "All async test functions and fixtures use asyncio_mode=auto — no pytest.mark.asyncio needed"
    - "In-memory SQLite fixtures create and tear down cleanly between each test — no test pollution"
    - "The setup wizard integration test creates config.json, encrypts secrets, and reloads them correctly"
    - "The WAL integration test confirms PRAGMA journal_mode = 'wal' and passes 10 concurrent async writes"
    - "The Alembic integration test confirms all 11 tables exist after upgrade head"
    - "Performance tests pass: LLMClient init <500ms, config load <100ms, DB transaction <50ms"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Shared fixtures: db_session, mock_llm_client, config_dir, agent_deps, event_loop"
      exports: ["db_session", "mock_llm_client", "config_dir", "agent_deps"]
    - path: "tests/fixtures/yc_companies.json"
      provides: "100 stable YC company records for Scout tests"
      contains: "\"company_name\""
    - path: "tests/fixtures/user_profile.json"
      provides: "Standard UserProfile test data"
      contains: "\"name\", \"skills\""
    - path: "tests/fixtures/intel_brief.json"
      provides: "Standard IntelBrief test data"
      contains: "\"talking_points\""
    - path: "tests/unit/test_crypto.py"
      provides: "Covers TEST-P1-01: encryption roundtrip, key derivation determinism"
    - path: "tests/unit/test_db_models.py"
      provides: "Covers TEST-P1-02: all 11 SQLModel schemas serialize/deserialize"
    - path: "tests/unit/test_llm_client.py"
      provides: "Covers TEST-P1-03: all backends, tool call path, JSON path, XML fallback"
    - path: "tests/integration/test_db_wal.py"
      provides: "Covers TEST-P1-07: WAL mode enabled, concurrent writes pass"
    - path: "tests/integration/test_alembic_migration.py"
      provides: "Covers TEST-P1-08: all 11 tables present after upgrade head"
  key_links:
    - from: "tests/conftest.py"
      to: "ingot.db.engine"
      via: "db_session fixture creates in-memory SQLite engine, runs init_db, yields session, disposes"
      pattern: "sqlite\\+aiosqlite:///\\:memory\\:"
    - from: "tests/conftest.py"
      to: "pydantic_ai.models.test.TestModel"
      via: "mock_llm_client fixture uses TestModel; ALLOW_MODEL_REQUESTS=False ensures no real calls"
      pattern: "ALLOW_MODEL_REQUESTS.*False|TestModel"
    - from: "tests/conftest.py"
      to: "ingot.config.manager.ConfigManager"
      via: "config_dir fixture passes tmp_path to ConfigManager(base_dir=tmp_path)"
      pattern: "ConfigManager\\(base_dir"
---

<objective>
Build the complete Phase 1 test suite covering all TEST-P1-* and TEST-INFRA-* requirements. This is the quality gate for the entire phase.

Purpose: Without tests, there's no confidence that the config, DB, LLM, and agent framework actually work together. The test suite is the executable specification for Phase 1.
Output: Full pytest suite running in <30 seconds, zero API calls, ≥70% overall coverage, ≥80% on critical paths.
</objective>

<execution_context>
@/Users/ishansingh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ishansingh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/01-foundation-and-core-infrastructure/01-RESEARCH.md
@.planning/phases/01-foundation-and-core-infrastructure/01-01-SUMMARY.md
@.planning/phases/01-foundation-and-core-infrastructure/01-02-SUMMARY.md
@.planning/phases/01-foundation-and-core-infrastructure/01-03-SUMMARY.md
@.planning/phases/01-foundation-and-core-infrastructure/01-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test infrastructure — conftest, fixtures, and fixture data files</name>
  <files>
    tests/__init__.py
    tests/conftest.py
    tests/fixtures/yc_companies.json
    tests/fixtures/user_profile.json
    tests/fixtures/intel_brief.json
    tests/unit/__init__.py
    tests/integration/__init__.py
  </files>
  <action>
**CRITICAL pitfall (Pitfall 5 from RESEARCH.md):** With `asyncio_mode = "auto"` in pyproject.toml, async fixtures MUST use `@pytest_asyncio.fixture`, NOT `@pytest.fixture`. Using the wrong decorator causes the fixture to return a coroutine object instead of the awaited value. Always use `import pytest_asyncio` and `@pytest_asyncio.fixture` for every async fixture.

**tests/conftest.py** — shared fixtures used by all tests:

```python
import pytest
import pytest_asyncio
import tempfile
import pathlib
from unittest.mock import AsyncMock, MagicMock
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlmodel import SQLModel
from pydantic_ai import models
from pydantic_ai.models.test import TestModel

# Prevent any real LLM calls in tests — fail loudly if attempted
models.ALLOW_MODEL_REQUESTS = False


@pytest_asyncio.fixture
async def db_session():
    """
    In-memory SQLite session. Created fresh for each test, torn down after.
    Uses aiosqlite — same driver as production but in-memory so no disk I/O.
    """
    from ingot.db.engine import create_engine, init_db
    from ingot.db.models import *  # noqa: F401, F403 — registers all models in metadata

    engine = create_engine("sqlite+aiosqlite:///:memory:")
    await init_db(engine)
    Session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
    async with Session() as session:
        yield session
    await engine.dispose()


@pytest.fixture
def tmp_config_dir(tmp_path):
    """
    Temporary config directory. Passed as base_dir to ConfigManager.
    ConfigManager creates all subdirs itself. Cleaned up by tmp_path fixture.
    """
    return tmp_path


@pytest.fixture
def config_manager(tmp_config_dir):
    """Configured ConfigManager pointing to tmp_path. No ~/.outreach-agent/ pollution."""
    from ingot.config.manager import ConfigManager
    cm = ConfigManager(base_dir=tmp_config_dir)
    cm.ensure_dirs()
    return cm


@pytest.fixture
def mock_llm_client():
    """
    PydanticAI TestModel-backed LLMClient mock.
    Use scout_agent.override(model=TestModel()) in per-test context if you need
    to override output. This fixture provides the LLMClient wrapper for AgentDeps.
    """
    from ingot.llm.client import LLMClient
    client = MagicMock(spec=LLMClient)
    client.model = "test-model"
    client.complete = AsyncMock(return_value=None)  # Override in each test
    return client


@pytest_asyncio.fixture
async def http_client():
    """Fresh httpx.AsyncClient for tests. Not the singleton — avoids cross-test pollution."""
    import httpx
    async with httpx.AsyncClient() as client:
        yield client


@pytest_asyncio.fixture
async def agent_deps(db_session, mock_llm_client, http_client):
    """
    Fully assembled AgentDeps for agent tests.
    Injects in-memory DB session, mock LLM client, and real httpx client.
    """
    from ingot.agents.base import AgentDeps
    return AgentDeps(
        llm_client=mock_llm_client,
        session=db_session,
        http_client=http_client,
        verbosity=0,
        agent_name="test",
    )
```

**tests/fixtures/yc_companies.json** — 100 stable YC company records. Generate realistic but fictional data:
```json
[
  {
    "company_name": "Stripe",
    "person_name": "Patrick Collison",
    "person_email": "patrick@stripe.com",
    "person_role": "CEO",
    "company_website": "https://stripe.com",
    "source_venue": "yc",
    "batch": "S09"
  },
  ... (100 records total)
]
```
Use well-known real YC companies for the first 10 (Stripe, Airbnb, Dropbox, Instacart, Coinbase, DoorDash, Brex, Gusto, PagerDuty, Segment), then generate 90 fictional entries with realistic startup names, roles, and domains.

**tests/fixtures/user_profile.json** — standard UserProfile test data matching DB-01 schema exactly:
```json
{
  "name": "Alex Chen",
  "headline": "Senior Software Engineer | Python, distributed systems, ML infrastructure",
  "skills": ["Python", "FastAPI", "PostgreSQL", "Redis", "Kubernetes", "PyTorch", "asyncio"],
  "experience": [
    {
      "company": "TechCorp",
      "role": "Senior Software Engineer",
      "duration": "2021-2024",
      "description": "Built distributed data pipeline processing 10M events/day"
    }
  ],
  "education": [{"school": "MIT", "degree": "BS Computer Science", "year": "2019"}],
  "projects": [{"name": "async-cache", "description": "Redis-backed async cache layer", "url": "https://github.com/alexchen/async-cache"}],
  "github_url": "https://github.com/alexchen",
  "linkedin_url": "https://linkedin.com/in/alexchen",
  "resume_raw_text": "Alex Chen\nSenior Software Engineer..."
}
```

**tests/fixtures/intel_brief.json** — standard IntelBrief test data matching DB-03 schema:
```json
{
  "company_name": "Stripe",
  "company_signals": ["Series H funding ($6.5B)", "Expanding into Asia-Pacific", "Hiring 500 engineers"],
  "person_name": "Patrick Collison",
  "person_role": "CEO",
  "company_website": "https://stripe.com",
  "person_background": "Founded Stripe at 22. MIT dropout. Known for deep technical involvement.",
  "talking_points": [
    "Stripe's expansion into APAC aligns with Alex's distributed systems experience",
    "Pattern matching between Alex's async data pipeline work and Stripe's payment processing at scale",
    "Alex's open-source async-cache library demonstrates the infrastructure thinking Stripe values"
  ],
  "company_product_description": "Online payment infrastructure for internet businesses"
}
```

Create `tests/__init__.py`, `tests/unit/__init__.py`, `tests/integration/__init__.py` as empty files.
  </action>
  <verify>
    <automated>pytest tests/conftest.py --collect-only -q 2>&1 | head -20 && echo "Conftest collectable OK"</automated>
  </verify>
  <done>
    `pytest --collect-only` finds and lists fixtures from conftest.py without errors. All three fixture JSON files exist with correct structure. `models.ALLOW_MODEL_REQUESTS = False` is set at conftest import time.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests (TEST-P1-01 through TEST-P1-09, TEST-INFRA-01 through TEST-INFRA-05)</name>
  <files>
    tests/unit/test_crypto.py
    tests/unit/test_config.py
    tests/unit/test_llm_client.py
    tests/unit/test_pydantic_validation.py
    tests/unit/test_retry.py
    tests/unit/test_db_models.py
    tests/unit/test_dispatcher.py
    tests/unit/test_http_client.py
    tests/unit/test_agent_framework.py
    tests/unit/test_agent_imports.py
    tests/unit/test_import_boundaries.py
    tests/unit/test_exceptions.py
    tests/unit/test_performance.py
  </files>
  <action>
Write all unit test files. Each file should be focused and complete. Use the test map from RESEARCH.md Validation Architecture section to know exactly which requirement each test covers.

**tests/unit/test_crypto.py** — covers TEST-P1-01 (INFRA-02, INFRA-03):
- `test_fernet_roundtrip`: encrypt_secret('hello') → decrypt_secret → 'hello'
- `test_key_derivation_deterministic`: call get_fernet() twice, verify both produce identical encrypted/decryptable output
- `test_machine_key_created_on_first_run`: with tmp_path, verify .key file created, chmod 600
- `test_invalid_token_raises`: decrypt_secret('garbage') raises exception (InvalidToken or ConfigError)
- `test_encrypt_empty_string`: edge case — empty string should round-trip correctly

**tests/unit/test_config.py** — covers INFRA-01, INFRA-05, INFRA-06, TEST-INFRA-04:
- `test_config_dir_created`: config_manager fixture creates base_dir with logs/, resume/, venues/ subdirs
- `test_default_config_loaded`: ConfigManager(base_dir=tmp_path).load() returns AppConfig with 7 agents
- `test_preset_fully_free`: verify all 7 agents set to 'ollama/llama3.1'
- `test_preset_best_quality`: verify writer/research use claude-3-5-sonnet, others use haiku
- `test_config_persist_reload`: set smtp.password, save, reload → password matches
- `test_secrets_encrypted_on_disk`: raw JSON has '__encrypted__:' prefix, not plaintext
- `test_per_agent_model_config`: set scout.model = 'gpt-4o', save, reload → scout.model == 'gpt-4o'

**tests/unit/test_llm_client.py** — covers INFRA-10, INFRA-11, INFRA-12, INFRA-13, INFRA-16, TEST-P1-03:
- `test_tool_call_path`: mock acompletion returning tool_calls → Pydantic schema returned
- `test_json_content_path`: mock acompletion returning JSON content → schema returned
- `test_xml_fallback_path`: mock acompletion returning XML content → xml_extract called → schema returned
- `test_xml_fallback_list_field`: XML with multiline list content → list[str] field populated
- `test_per_agent_model_string`: LLMClient('ollama/llama3.1') stores model, LLMClient('gpt-4o') stores gpt-4o
- `test_markdown_json_stripped`: content is ```json {...}``` → markdown stripped before JSON parse

**tests/unit/test_pydantic_validation.py** — covers INFRA-14, TEST-P1-04:
- `test_valid_response_accepted`: model_validate_json on valid JSON returns schema instance
- `test_missing_required_field_raises_llm_validation_error`: missing required field → LLMValidationError
- `test_wrong_type_raises_llm_validation_error`: wrong field type → LLMValidationError
- `test_error_has_raw_content`: LLMValidationError.raw_content is populated

**tests/unit/test_retry.py** — covers INFRA-15, TEST-P1-05:
- `test_retry_three_times_then_raises`: AsyncMock raises LLMError for first 2 calls, succeeds on 3rd
- `test_retry_exhausted_raises_llm_error`: AsyncMock always raises → LLMError raised after 3 attempts
- `test_no_retry_on_validation_error`: LLMValidationError is NOT retried (verify call_count == 1)
- `test_backoff_wait_called`: monkeypatch time/sleep to confirm exponential wait is applied

**tests/unit/test_db_models.py** — covers DB-01 through DB-11, TEST-P1-02:
- One test per model: `test_{model_name}_create_and_query` — creates instance, commits, queries back, asserts fields
- `test_lead_status_enum`: Lead with status='discovered' saves and reloads as 'discovered'
- `test_intel_brief_json_fields`: talking_points=['a', 'b'] saves and reloads as list[str]
- `test_foreign_key_relationship`: create Lead, create IntelBrief with lead_id, query IntelBrief

**tests/unit/test_dispatcher.py** — covers INFRA-17:
- `test_queue_drains_all_tasks`: enqueue 10 tasks, run_all() → 10 results
- `test_concurrent_workers`: 3 workers, 9 tasks → all complete, none dropped
- `test_failed_task_captured`: task that raises → TaskResult.success=False, error populated
- `test_empty_queue`: run_all() on empty queue → empty list

**tests/unit/test_http_client.py** — covers INFRA-18:
- `test_singleton_pattern`: get_http_client() called twice → same object
- `test_connection_pool_configured`: client.limits.max_connections == 10, max_keepalive_connections == 5
- `test_user_agent_set`: client.headers['user-agent'] contains 'INGOT'

**tests/unit/test_agent_framework.py** — covers AGENT-02, AGENT-06, TEST-INFRA-03:
- `test_all_agents_importable`: import all 7 agent modules without error
- `test_agent_deps_dataclass`: AgentDeps(llm_client=..., session=..., http_client=...) instantiates
- `test_pydantic_ai_agent_instantiates`: scout_agent is a PydanticAI Agent instance
- `test_testmodel_fixture`: with scout_agent.override(model=TestModel()), run returns without real API call

**tests/unit/test_agent_imports.py** — covers AGENT-01:
- `test_all_seven_agent_modules_importable`: all 7 agent module imports succeed
- `test_registry_contains_six_agents`: after importing agents, AGENT_REGISTRY has scout/research/matcher/writer/outreach/analyst

**tests/unit/test_import_boundaries.py** — covers AGENT-05, INFRA-11:
- `test_no_cross_agent_imports`: AST scan of each agent module, no imports from other agent modules
- `test_no_direct_anthropic_imports`: AST scan of entire src/ingot tree, no `import anthropic`
- `test_no_direct_openai_imports`: AST scan, no `import openai`

**tests/unit/test_exceptions.py** — covers AGENT-09:
- `test_exception_hierarchy`: LLMError/DBError/ConfigError/ValidationError/AgentError all subclass IngotError
- `test_cause_chaining`: raise LLMError('msg', cause=ValueError('orig')), str() contains both
- `test_agent_error_includes_name`: AgentError('Scout', 'failed') str contains '[Scout]'
- `test_never_swallow_bare_exception`: verify LLMClient raises LLMError not bare Exception on failure

**tests/unit/test_performance.py** — covers TEST-P1-09 (performance benchmarks):
- `test_llm_client_init_under_500ms`: `time.perf_counter()` around LLMClient('ollama/llama3.1') init
- `test_config_load_under_100ms`: `time.perf_counter()` around ConfigManager(base_dir=tmp).load()
- `test_db_transaction_under_50ms`: `time.perf_counter()` around `session.add(lead); await session.commit()`

For all performance tests: use `pytest.skip()` with a warning (not `assert`) if performance exceeds threshold in CI — flaky performance tests are worse than no performance tests. But DO measure and print the timing.

**TEST-INFRA-05 (coverage):** Already configured in pyproject.toml `--cov-fail-under=70`. The full suite must pass this gate.

**TEST-INFRA-06 (mock Gmail SMTP/IMAP):** No real SMTP/IMAP calls happen in Phase 1 (stubs only). Create placeholder test:
```python
# tests/unit/test_smtp_imap_stubs.py
def test_aiosmtplib_importable():
    import aiosmtplib
    assert aiosmtplib is not None

def test_aioimaplib_importable():
    import aioimaplib
    assert aioimaplib is not None
```
This satisfies TEST-INFRA-06 at the stub level. Phase 3 will add actual SMTP/IMAP mock tests.
  </action>
  <verify>
    <automated>pytest tests/unit/ -x -q --tb=short 2>&1 | tail -20</automated>
  </verify>
  <done>
    `pytest tests/unit/ -x -q` passes with 0 errors. All 13 unit test files have tests that pass. Performance tests print timing and pass (or skip with warning in slow environments).
  </done>
</task>

<task type="auto">
  <name>Task 3: Integration tests and full suite gate (TEST-P1-06, TEST-P1-07, TEST-P1-08)</name>
  <files>
    tests/integration/test_setup_wizard.py
    tests/integration/test_db_wal.py
    tests/integration/test_alembic_migration.py
  </files>
  <action>
**tests/integration/test_setup_wizard.py** — covers INFRA-04, TEST-P1-06:

```python
import pytest
import pathlib
import json
from ingot.config.manager import ConfigManager
from ingot.config.schema import AppConfig

def test_setup_wizard_non_interactive_creates_config(tmp_path, monkeypatch):
    """Full integration: env vars → setup wizard → config.json created → reloads correctly."""
    monkeypatch.setenv("ANTHROPIC_API_KEY", "sk-ant-test-key")
    monkeypatch.setenv("GMAIL_USERNAME", "test@gmail.com")
    monkeypatch.setenv("GMAIL_APP_PASSWORD", "test-app-password")
    monkeypatch.setenv("INGOT_BASE_DIR", str(tmp_path))  # Override home dir for test

    from typer.testing import CliRunner
    from ingot.cli import app
    runner = CliRunner()
    result = runner.invoke(app, ["setup", "--non-interactive", "--preset", "fully_free"])
    assert result.exit_code == 0, f"Exit code {result.exit_code}: {result.output}"

    # Verify config.json exists and was written
    config_path = tmp_path / "config.json"
    assert config_path.exists()

    # Verify secrets are encrypted on disk
    raw = json.loads(config_path.read_text())
    assert "__encrypted__:" in raw["smtp"]["password"]

    # Verify reload decrypts correctly
    cm = ConfigManager(base_dir=tmp_path)
    cfg = cm.load()
    assert cfg.smtp.password == "test-app-password"
    assert cfg.smtp.username == "test@gmail.com"
    assert len(cfg.agents) == 7
    assert all(a.model == "ollama/llama3.1" for a in cfg.agents.values())

def test_setup_wizard_reruns_skip_existing_values(tmp_path, monkeypatch):
    """Re-running wizard skips fields already configured."""
    monkeypatch.setenv("INGOT_BASE_DIR", str(tmp_path))
    monkeypatch.setenv("GMAIL_USERNAME", "first@gmail.com")
    monkeypatch.setenv("GMAIL_APP_PASSWORD", "first-password")

    from typer.testing import CliRunner
    from ingot.cli import app
    runner = CliRunner()

    # First run
    runner.invoke(app, ["setup", "--non-interactive", "--preset", "fully_free"])

    # Second run with different email — should skip already-configured username
    monkeypatch.setenv("GMAIL_USERNAME", "second@gmail.com")
    runner.invoke(app, ["setup", "--non-interactive"])

    cm = ConfigManager(base_dir=tmp_path)
    cfg = cm.load()
    # Username was already set — second run should not overwrite
    assert cfg.smtp.username == "first@gmail.com"
```

**Note on INGOT_BASE_DIR:** If ConfigManager doesn't support env var override yet, add support: `base_dir = Path(os.environ.get("INGOT_BASE_DIR", str(Path.home() / ".outreach-agent")))`. This is needed for tests to not pollute the real `~/.outreach-agent/`.

**tests/integration/test_db_wal.py** — covers INFRA-07, INFRA-08, TEST-P1-07:

```python
import pytest
import pytest_asyncio
import asyncio
from sqlalchemy import text

@pytest_asyncio.fixture
async def file_db_engine(tmp_path):
    """File-based SQLite for WAL tests (WAL mode only works with file-based DBs, not :memory:)."""
    from ingot.db.engine import create_engine, init_db
    from ingot.db.models import *  # noqa
    db_path = tmp_path / "test_wal.db"
    engine = create_engine(f"sqlite+aiosqlite:///{db_path}")
    await init_db(engine)
    yield engine
    await engine.dispose()

async def test_wal_mode_enabled(file_db_engine):
    async with file_db_engine.connect() as conn:
        result = await conn.execute(text("PRAGMA journal_mode"))
        mode = result.scalar()
        assert mode == "wal", f"Expected WAL mode, got: {mode}"

async def test_tables_created(file_db_engine):
    async with file_db_engine.connect() as conn:
        result = await conn.execute(text("SELECT name FROM sqlite_master WHERE type='table'"))
        tables = {row[0].lower() for row in result.fetchall()}
        expected = {"userprofile", "lead", "intelbrief", "match", "email",
                    "followup", "campaign", "agentlog", "venue",
                    "outreachmetric", "unsubscribedemail"}
        missing = expected - tables
        assert not missing, f"Missing tables: {missing}"

async def test_concurrent_writes_no_busy_error(file_db_engine):
    """10 concurrent writes to Lead table must not raise SQLITE_BUSY."""
    from ingot.db.models import Lead
    from sqlalchemy.orm import sessionmaker
    from sqlalchemy.ext.asyncio import AsyncSession

    Session = sessionmaker(file_db_engine, class_=AsyncSession, expire_on_commit=False)

    async def write_one(i: int):
        async with Session() as s:
            s.add(Lead(company_name=f"Company {i}"))
            await s.commit()

    await asyncio.gather(*[write_one(i) for i in range(10)])

    async with Session() as s:
        from sqlmodel import select
        result = await s.execute(select(Lead))
        count = len(result.all())
        assert count == 10, f"Expected 10 leads, got {count}"
```

**tests/integration/test_alembic_migration.py** — covers INFRA-09, TEST-P1-08:

```python
import pytest
import subprocess
import pathlib
import os
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy import text

async def test_alembic_upgrade_head_creates_all_tables(tmp_path):
    """alembic upgrade head from scratch produces all 11 tables."""
    db_path = tmp_path / "migration_test.db"
    env = {**os.environ, "INGOT_BASE_DIR": str(tmp_path)}

    # Run alembic upgrade head
    result = subprocess.run(
        ["alembic", "upgrade", "head"],
        capture_output=True, text=True, env=env, cwd=str(pathlib.Path.cwd())
    )
    assert result.returncode == 0, f"alembic upgrade head failed:\nSTDOUT: {result.stdout}\nSTDERR: {result.stderr}"

    # Connect and check tables
    engine = create_async_engine(f"sqlite+aiosqlite:///{db_path}")
    async with engine.connect() as conn:
        rows = await conn.execute(text("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'alembic%'"))
        tables = {row[0].lower() for row in rows.fetchall()}
    await engine.dispose()

    expected = {"userprofile", "lead", "intelbrief", "match", "email",
                "followup", "campaign", "agentlog", "venue",
                "outreachmetric", "unsubscribedemail"}
    missing = expected - tables
    assert not missing, f"Missing tables after migration: {missing}"

async def test_alembic_downgrade_then_upgrade(tmp_path):
    """Downgrade to base then upgrade head — schema must be idempotent."""
    env = {**os.environ, "INGOT_BASE_DIR": str(tmp_path)}
    cwd = str(pathlib.Path.cwd())

    subprocess.run(["alembic", "upgrade", "head"], env=env, cwd=cwd, check=True)
    subprocess.run(["alembic", "downgrade", "base"], env=env, cwd=cwd, check=True)
    result = subprocess.run(["alembic", "upgrade", "head"], env=env, cwd=cwd, capture_output=True, text=True)
    assert result.returncode == 0, f"Re-upgrade failed: {result.stderr}"
```

**Run the full suite after integration tests are written:**
```bash
pytest tests/ -x -q --cov=ingot --cov-report=term-missing --cov-fail-under=70
```

If coverage is below 70%, identify the uncovered modules and add targeted tests. Priority coverage: `ingot.config.crypto` (80%+), `ingot.db.engine` (80%+), `ingot.llm.client` (80%+).
  </action>
  <verify>
    <automated>pytest tests/ -x -q --cov=ingot --cov-report=term-missing --cov-fail-under=70 2>&1 | tail -30</automated>
  </verify>
  <done>
    `pytest tests/ --cov=ingot --cov-fail-under=70` exits with code 0. All integration tests pass. WAL mode confirmed in test_db_wal.py. Alembic migration creates all 11 tables. Setup wizard integration test creates and reloads config correctly. Total suite completes in under 30 seconds.
  </done>
</task>

</tasks>

<verification>
Final phase gate — run after all 3 tasks complete:

```bash
# Full suite with coverage
pytest tests/ -x -q --cov=ingot --cov-report=term-missing --cov-fail-under=70

# Confirm test timing
time pytest tests/ -q --no-header

# Confirm zero real API calls (ALLOW_MODEL_REQUESTS=False would cause error if hit)
# If the suite passes, no real API calls were made.

# Specific requirement checks
pytest tests/unit/test_crypto.py -v          # TEST-P1-01
pytest tests/unit/test_db_models.py -v       # TEST-P1-02
pytest tests/unit/test_llm_client.py -v      # TEST-P1-03
pytest tests/unit/test_pydantic_validation.py -v  # TEST-P1-04
pytest tests/unit/test_retry.py -v           # TEST-P1-05
pytest tests/integration/test_setup_wizard.py -v  # TEST-P1-06
pytest tests/integration/test_db_wal.py -v   # TEST-P1-07
pytest tests/integration/test_alembic_migration.py -v  # TEST-P1-08
pytest tests/unit/test_performance.py -v -s  # TEST-P1-09 (show timings with -s)
```
</verification>

<success_criteria>
- `pytest tests/ -x -q --cov=ingot --cov-fail-under=70` exits 0
- Total suite runtime under 30 seconds (per user's testing philosophy decision)
- Zero real LLM API calls — `models.ALLOW_MODEL_REQUESTS = False` enforced
- All 9 TEST-P1-* requirements covered by automated tests
- All 8 TEST-INFRA-* requirements covered (infrastructure, fixtures, mocking, coverage)
- Integration test confirms WAL mode is active (PRAGMA journal_mode = 'wal')
- Integration test confirms all 11 tables present after alembic upgrade head
- Integration test confirms setup wizard creates encrypted config and reloads correctly
- Performance: LLMClient init <500ms, config load <100ms, DB transaction <50ms
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-and-core-infrastructure/01-05-SUMMARY.md` with:
- Final test count (unit + integration)
- Actual coverage percentage (overall and per-module)
- Actual suite runtime
- Any tests that were skipped or marked xfail, and why
- Fixture patterns established (for Phase 2 tests to extend)
</output>
