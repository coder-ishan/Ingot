---
phase: 01-foundation-and-core-infrastructure
plan: 03
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - src/ingot/llm/__init__.py
  - src/ingot/llm/client.py
  - src/ingot/llm/fallback.py
  - src/ingot/llm/schemas.py
  - src/ingot/agents/exceptions.py
autonomous: true
requirements:
  - INFRA-10
  - INFRA-11
  - INFRA-12
  - INFRA-13
  - INFRA-14
  - INFRA-15
  - INFRA-16

must_haves:
  truths:
    - "LLMClient.complete() accepts model strings for Claude, OpenAI, Ollama, and any OpenAI-compatible API"
    - "LLMClient.complete() retries exactly 3 times with exponential backoff on transient failures before raising LLMError"
    - "When tool calls return valid JSON, LLMClient parses and validates against the Pydantic response_schema"
    - "When tool calls fail or return invalid JSON, LLMClient falls back to XML tag extraction and validates via Pydantic"
    - "Invalid LLM responses (both JSON and XML paths) raise LLMValidationError with a descriptive message — never silently return None"
    - "No agent module imports anthropic or openai directly; LLMClient is the only LLM entry point"
    - "Typed exception hierarchy exists: IngotError → LLMError, LLMValidationError, DBError, ConfigError, ValidationError"
  artifacts:
    - path: "src/ingot/llm/client.py"
      provides: "LLMClient with complete(), retry, Pydantic validation, XML fallback"
      exports: ["LLMClient"]
    - path: "src/ingot/llm/fallback.py"
      provides: "XML tag extraction fallback parser"
      exports: ["xml_extract"]
    - path: "src/ingot/llm/schemas.py"
      provides: "Pydantic models for LLM request/response envelopes"
      exports: ["LLMRequest", "LLMResponse"]
    - path: "src/ingot/agents/exceptions.py"
      provides: "Full typed exception hierarchy for INGOT"
      exports: ["IngotError", "LLMError", "LLMValidationError", "DBError", "ConfigError", "ValidationError", "AgentError"]
  key_links:
    - from: "src/ingot/llm/client.py"
      to: "litellm.acompletion"
      via: "single call site in LLMClient.complete() — no direct anthropic/openai imports anywhere"
      pattern: "from litellm import acompletion"
    - from: "src/ingot/llm/client.py"
      to: "src/ingot/llm/fallback.py"
      via: "xml_extract() called when tool_calls is None and JSON parse fails"
      pattern: "xml_extract\\("
    - from: "src/ingot/llm/client.py"
      to: "pydantic BaseModel.model_validate"
      via: "every response path ends with response_schema.model_validate() before returning"
      pattern: "model_validate"
---

<objective>
Build LLMClient — the single, unified LLM abstraction that all 7 agents use. LiteLLM routes to any backend; tenacity handles retries; XML fallback ensures Ollama models without tool-call support still work.

Purpose: Without this layer, agents would import anthropic/openai directly, making per-agent backend configuration impossible and making tests require real API keys. This plan eliminates both problems.
Output: `ingot.llm` package with LLMClient, XML fallback parser, and typed exception hierarchy.
</objective>

<execution_context>
@/Users/ishansingh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ishansingh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/01-foundation-and-core-infrastructure/01-RESEARCH.md
@.planning/phases/01-foundation-and-core-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Typed exception hierarchy</name>
  <files>
    src/ingot/agents/__init__.py
    src/ingot/agents/exceptions.py
  </files>
  <action>
Create the typed exception hierarchy referenced throughout the codebase. This is needed before LLMClient can raise typed errors.

**src/ingot/agents/exceptions.py:**

```python
"""
INGOT typed exception hierarchy.

Rule: Never raise bare Exception. Always raise the most specific subclass.
Callers must catch specific types — catching IngotError is only acceptable
at the top-level CLI handler that formats user-visible error messages.
"""

class IngotError(Exception):
    """Base exception for all INGOT errors. Carry a user-friendly message."""
    def __init__(self, message: str, *, cause: Exception | None = None):
        super().__init__(message)
        self.message = message
        self.cause = cause

    def __str__(self) -> str:
        if self.cause:
            return f"{self.message} (caused by: {type(self.cause).__name__}: {self.cause})"
        return self.message


class LLMError(IngotError):
    """LLM backend unreachable, timeout, or all retries exhausted."""
    pass


class LLMValidationError(IngotError):
    """LLM returned a response that failed Pydantic validation."""
    def __init__(self, message: str, *, raw_content: str = "", cause: Exception | None = None):
        super().__init__(message, cause=cause)
        self.raw_content = raw_content


class DBError(IngotError):
    """Database read/write failure. Best-effort recovery may apply."""
    pass


class ConfigError(IngotError):
    """Configuration missing, invalid, or encryption key lost."""
    pass


class ValidationError(IngotError):
    """Input data failed schema validation (distinct from LLM response validation)."""
    pass


class AgentError(IngotError):
    """Agent-level failure (agent-specific logic error, not LLM or DB)."""
    def __init__(self, agent_name: str, message: str, *, cause: Exception | None = None):
        super().__init__(f"[{agent_name}] {message}", cause=cause)
        self.agent_name = agent_name
```

**src/ingot/agents/__init__.py** — export the exception classes and a stub `AGENT_REGISTRY = {}` dict for Plan 01-04.

Also update `src/ingot/config/crypto.py` from Plan 01-01: replace the local `class ConfigError` stub with `from ingot.agents.exceptions import ConfigError`. This removes the temporary stub.
  </action>
  <verify>
    <automated>python -c "
from ingot.agents.exceptions import (
    IngotError, LLMError, LLMValidationError,
    DBError, ConfigError, ValidationError, AgentError
)
# Verify hierarchy
assert issubclass(LLMError, IngotError)
assert issubclass(LLMValidationError, IngotError)
assert issubclass(DBError, IngotError)
assert issubclass(ConfigError, IngotError)

# Verify cause chaining
try:
    raise ValueError('original')
except ValueError as e:
    err = LLMError('wrapped', cause=e)
    assert 'wrapped' in str(err)
    assert 'ValueError' in str(err)

# Verify AgentError includes agent name
err = AgentError('Scout', 'fetch failed')
assert '[Scout]' in str(err)
print('Exception hierarchy OK')
"
    </automated>
  </verify>
  <done>
    All exception classes exist, are importable, and form a correct hierarchy under `IngotError`. `AgentError` includes agent name in message. `cause` chaining works.
  </done>
</task>

<task type="auto">
  <name>Task 2: LLMClient with LiteLLM, retry, Pydantic validation, and XML fallback</name>
  <files>
    src/ingot/llm/__init__.py
    src/ingot/llm/client.py
    src/ingot/llm/fallback.py
    src/ingot/llm/schemas.py
  </files>
  <action>
**CRITICAL anti-patterns to avoid:**
- Do NOT import `anthropic` or `openai` anywhere — only `from litellm import acompletion`
- Do NOT use `litellm.api_key = ...` globals — pass credentials via env vars or per-call
- Do NOT swallow exceptions and return None — always raise typed errors after retry exhaustion
- Do NOT use `result_type=` in PydanticAI (v0.x API) — this plan uses LiteLLM directly

**src/ingot/llm/fallback.py:**

```python
"""XML tag extraction fallback for LLM models that don't support structured tool calls."""
import re
from typing import TypeVar, Type
from pydantic import BaseModel
from ingot.agents.exceptions import LLMValidationError

T = TypeVar("T", bound=BaseModel)

def xml_extract(content: str, schema: Type[T]) -> T:
    """
    Extract field values from XML-like tags in LLM text output.

    Example input:
        <company_name>Acme Corp</company_name>
        <person_name>Jane Doe</person_name>

    Extracts each field named in schema.model_fields. List fields expected as
    newline-separated values inside the tag. Nested objects not supported —
    use flat schemas for XML fallback paths.
    """
    data = {}
    for field_name, field_info in schema.model_fields.items():
        pattern = rf"<{field_name}>(.*?)</{field_name}>"
        match = re.search(pattern, content, re.DOTALL)
        if match:
            raw_value = match.group(1).strip()
            # Detect list fields by annotation
            annotation = field_info.annotation
            origin = getattr(annotation, "__origin__", None)
            if origin is list:
                data[field_name] = [line.strip() for line in raw_value.splitlines() if line.strip()]
            else:
                data[field_name] = raw_value
    try:
        return schema.model_validate(data)
    except Exception as e:
        raise LLMValidationError(
            f"XML fallback validation failed for {schema.__name__}: {e}",
            raw_content=content,
            cause=e,
        ) from e
```

**src/ingot/llm/schemas.py:**

```python
from pydantic import BaseModel

class LLMMessage(BaseModel):
    role: str  # "system" | "user" | "assistant"
    content: str

class LLMRequest(BaseModel):
    model: str
    messages: list[LLMMessage]
    tools: list[dict] | None = None

class LLMResponse(BaseModel):
    """Internal envelope — not returned to callers; they get the validated schema instance."""
    content: str
    tool_call_args: str | None = None  # JSON string if tool call
    finish_reason: str
    used_xml_fallback: bool = False
```

**src/ingot/llm/client.py** — implement per RESEARCH.md Pattern 3, with these additions:

```python
from typing import TypeVar, Type
from tenacity import (
    retry, stop_after_attempt, wait_exponential,
    retry_if_exception_type, before_sleep_log
)
import logging
from litellm import acompletion
from pydantic import BaseModel
from ingot.agents.exceptions import LLMError, LLMValidationError
from ingot.llm.fallback import xml_extract

T = TypeVar("T", bound=BaseModel)
logger = logging.getLogger("ingot.llm")

class LLMClient:
    def __init__(self, model: str, max_retries: int = 3):
        self.model = model
        self.max_retries = max_retries

    async def complete(
        self,
        messages: list[dict],
        response_schema: Type[T],
        tools: list[dict] | None = None,
        *,
        use_xml_fallback: bool = True,
    ) -> T:
        """
        Call LLM and return a validated Pydantic instance.

        Retry strategy: 3 attempts, exponential backoff (2s, 4s, 8s).
        Response path priority:
          1. Native tool call (finish_reason == 'tool_calls') → JSON parse → Pydantic validate
          2. Content as JSON → Pydantic validate
          3. XML tag extraction (if use_xml_fallback=True) → Pydantic validate
        Raises LLMError on backend failure after all retries.
        Raises LLMValidationError if all response paths fail Pydantic validation.
        """
        return await self._complete_with_retry(messages, response_schema, tools, use_xml_fallback)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=30),
        retry=retry_if_exception_type(LLMError),
        reraise=True,
    )
    async def _complete_with_retry(self, messages, response_schema, tools, use_xml_fallback):
        try:
            kwargs = {"model": self.model, "messages": messages}
            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"

            response = await acompletion(**kwargs)
            raw = response.choices[0].message
            finish_reason = response.choices[0].finish_reason or ""

            # Path 1: Native tool call
            if raw.tool_calls:
                args_json = raw.tool_calls[0].function.arguments
                try:
                    return response_schema.model_validate_json(args_json)
                except Exception as e:
                    logger.debug("Tool call JSON validation failed, trying content fallback: %s", e)

            # Path 2: Content as JSON
            content = raw.content or ""
            if content:
                # Strip markdown code blocks if present (```json ... ```)
                import re
                json_match = re.search(r"```(?:json)?\s*([\s\S]*?)```", content)
                json_str = json_match.group(1).strip() if json_match else content.strip()
                try:
                    return response_schema.model_validate_json(json_str)
                except Exception:
                    pass  # fall through to XML

            # Path 3: XML fallback
            if use_xml_fallback and content:
                return xml_extract(content, response_schema)

            raise LLMValidationError(
                f"LLM response could not be parsed for schema {response_schema.__name__}",
                raw_content=content,
            )

        except (LLMValidationError, LLMError):
            raise  # Don't wrap these — they're already typed
        except Exception as e:
            raise LLMError(f"LLM backend error: {e}", cause=e) from e
```

**Verbosity integration:** When instantiated with `verbosity >= 1`, log retry attempts with `[AgentName] Retrying LLM call (attempt N/3)...` format. Use `before_sleep_log` from tenacity or a custom `before_sleep` callback.

**Per-agent model support:** `LLMClient` accepts the model string directly at construction. Agents are constructed by the Orchestrator (Plan 01-04) which reads `config.agents[agent_name].model` and passes it here.

**src/ingot/llm/__init__.py** — export `LLMClient`.
  </action>
  <verify>
    <automated>python -c "
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from pydantic import BaseModel
from ingot.llm.client import LLMClient
from ingot.llm.fallback import xml_extract
from ingot.agents.exceptions import LLMError, LLMValidationError

class TestSchema(BaseModel):
    company: str
    role: str

async def test_tool_call_path():
    client = LLMClient('ollama/llama3.1')
    mock_response = MagicMock()
    mock_response.choices[0].message.tool_calls = [MagicMock()]
    mock_response.choices[0].message.tool_calls[0].function.arguments = '{\"company\": \"Acme\", \"role\": \"Engineer\"}'
    mock_response.choices[0].message.content = None
    mock_response.choices[0].finish_reason = 'tool_calls'

    with patch('ingot.llm.client.acompletion', return_value=mock_response):
        result = await client.complete([{'role': 'user', 'content': 'test'}], TestSchema)
        assert result.company == 'Acme'
    print('Tool call path OK')

async def test_xml_fallback_path():
    client = LLMClient('ollama/llama3.1')
    mock_response = MagicMock()
    mock_response.choices[0].message.tool_calls = None
    mock_response.choices[0].message.content = '<company>Acme Corp</company><role>CTO</role>'
    mock_response.choices[0].finish_reason = 'stop'

    with patch('ingot.llm.client.acompletion', return_value=mock_response):
        result = await client.complete([{'role': 'user', 'content': 'test'}], TestSchema)
        assert result.company == 'Acme Corp'
        assert result.role == 'CTO'
    print('XML fallback path OK')

async def test_retry_on_transient_failure():
    client = LLMClient('claude/test', max_retries=3)
    call_count = 0

    async def flaky(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        if call_count < 3:
            raise ConnectionError('transient')
        mock_response = MagicMock()
        mock_response.choices[0].message.tool_calls = None
        mock_response.choices[0].message.content = '{\"company\": \"Retry Corp\", \"role\": \"CEO\"}'
        mock_response.choices[0].finish_reason = 'stop'
        return mock_response

    with patch('ingot.llm.client.acompletion', side_effect=flaky):
        result = await client.complete([{'role': 'user', 'content': 'test'}], TestSchema)
        assert result.company == 'Retry Corp'
        assert call_count == 3
    print('Retry path OK')

async def test_validation_error_on_bad_response():
    client = LLMClient('ollama/llama3.1')
    mock_response = MagicMock()
    mock_response.choices[0].message.tool_calls = None
    mock_response.choices[0].message.content = 'completely unparseable garbage'
    mock_response.choices[0].finish_reason = 'stop'

    with patch('ingot.llm.client.acompletion', return_value=mock_response):
        try:
            await client.complete([{'role': 'user', 'content': 'test'}], TestSchema)
            assert False, 'Should have raised'
        except LLMValidationError:
            pass  # expected
    print('Validation error path OK')

asyncio.run(test_tool_call_path())
asyncio.run(test_xml_fallback_path())
asyncio.run(test_retry_on_transient_failure())
asyncio.run(test_validation_error_on_bad_response())

# Verify no direct anthropic/openai imports exist
import ast, pathlib
for path in pathlib.Path('src/ingot').rglob('*.py'):
    tree = ast.parse(path.read_text())
    for node in ast.walk(tree):
        if isinstance(node, (ast.Import, ast.ImportFrom)):
            for alias in getattr(node, 'names', []):
                assert alias.name not in ('anthropic', 'openai'), f'Direct import found in {path}: {alias.name}'
            if hasattr(node, 'module') and node.module in ('anthropic', 'openai'):
                assert False, f'Direct import found in {path}: {node.module}'
print('No direct anthropic/openai imports OK')
print('All LLMClient tests passed')
"
    </automated>
  </verify>
  <done>
    LLMClient routes through tool calls → JSON → XML in priority order. Retry fires on transient LLMError (not on LLMValidationError). Invalid responses raise LLMValidationError. No `anthropic` or `openai` imports exist in `src/ingot/`. All INFRA-10 through INFRA-16 requirements are implemented.
  </done>
</task>

</tasks>

<verification>
Run after all tasks complete:

```bash
# Full verification sweep
python -c "
from ingot.agents.exceptions import IngotError, LLMError, LLMValidationError, DBError, ConfigError
from ingot.llm.client import LLMClient
from ingot.llm.fallback import xml_extract
from pydantic import BaseModel

class Schema(BaseModel):
    name: str
    value: str

# XML fallback test
result = xml_extract('<name>Test</name><value>42</value>', Schema)
assert result.name == 'Test'
assert result.value == '42'

# XML fallback with list field
from pydantic import Field
class ListSchema(BaseModel):
    items: list[str] = Field(default_factory=list)

result2 = xml_extract('<items>item1\nitem2\nitem3</items>', ListSchema)
assert result2.items == ['item1', 'item2', 'item3']

print('All verification checks passed')
"

# Verify import boundaries (no direct anthropic/openai in codebase)
python -c "
import ast, pathlib, sys
violations = []
for path in pathlib.Path('src/ingot').rglob('*.py'):
    try:
        tree = ast.parse(path.read_text())
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom):
                if node.module in ('anthropic', 'openai'):
                    violations.append(str(path))
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name in ('anthropic', 'openai'):
                        violations.append(str(path))
    except SyntaxError:
        pass
if violations:
    print('VIOLATIONS:', violations)
    sys.exit(1)
print('Import boundary check passed')
"
```
</verification>

<success_criteria>
- `LLMClient.complete()` works via all three response paths: tool calls, JSON content, XML fallback
- Retry fires exactly 3 times with exponential backoff on `LLMError`; does NOT retry on `LLMValidationError`
- `LLMValidationError` is raised (not `Exception`) when all response paths fail validation
- `xml_extract()` handles both scalar and list fields correctly
- Zero direct `anthropic` or `openai` imports anywhere in `src/ingot/`
- Full typed exception hierarchy under `IngotError` is importable and forms a correct inheritance tree
- All INFRA-10 through INFRA-16 requirements are addressed
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-and-core-infrastructure/01-03-SUMMARY.md` with:
- LLMClient interface (complete() signature, what it accepts, what it returns)
- Exception hierarchy listing
- XML fallback limitations (flat schemas only, list fields via newlines)
- Which tenacity parameters were used (for Plan 01-05 test setup)
</output>
