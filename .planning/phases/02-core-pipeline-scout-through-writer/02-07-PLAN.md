---
phase: 02-core-pipeline-scout-through-writer
plan: 07
type: tdd
wave: 5
depends_on:
  - "02-06"
files_modified:
  - tests/phase2/__init__.py
  - tests/phase2/conftest.py
  - tests/phase2/test_profile.py
  - tests/phase2/test_scout.py
  - tests/phase2/test_research.py
  - tests/phase2/test_matcher.py
  - tests/phase2/test_writer.py
  - tests/phase2/test_orchestrator.py
  - tests/phase2/test_integration.py
  - tests/phase2/fixtures/resume_sample.pdf
  - tests/phase2/fixtures/resume_sample.docx
  - tests/phase2/fixtures/yc_companies_fixture.json
autonomous: true
requirements:
  - TEST-P2-01
  - TEST-P2-02
  - TEST-P2-03
  - TEST-P2-04
  - TEST-P2-05
  - TEST-P2-06
  - TEST-P2-07
  - TEST-P2-08
  - TEST-P2-09
  - TEST-P2-10
  - TEST-P2-11
  - TEST-P2-12
  - TEST-P2-13
  - TEST-P2-14
  - TEST-P2-15
  - TEST-P2-16

must_haves:
  truths:
    - "All Phase 2 tests pass — pytest exits 0"
    - "Coverage on ingot.agents.*, ingot.venues.*, ingot.scoring.*, ingot.review.* meets minimum 70%"
    - "No test requires real API keys, real YC network access, or real LLM calls — all external calls are mocked"
    - "Scout performance test: score_lead() on 100 YC fixture companies completes in under 5 seconds"
    - "Research Phase 1 performance test: 5 fixture leads complete Phase 1 in under 10 seconds with TestModel"
    - "Match+Write performance test: 5 fixture leads through Matcher + Writer completes in under 15 seconds with TestModel"
    - "Orchestrator checkpoint/resume test: pipeline interrupted after Phase 1 and resumed produces no duplicate Lead records and no duplicate Email records"
  artifacts:
    - path: "tests/phase2/conftest.py"
      provides: "fixture_db (temp SQLite), fixture_leads (5 Lead records), fixture_user_profile (UserProfile schema), fixture_intel_brief (IntelBriefFull), fixture_yc_companies (100 company dicts), mock_http_client"
      exports: ["fixture_db", "fixture_leads", "fixture_user_profile", "fixture_intel_brief", "fixture_yc_companies", "mock_http_client"]
    - path: "tests/phase2/fixtures/yc_companies_fixture.json"
      provides: "100 realistic YC company records matching the yc-oss API schema for Scout tests"
      contains: "100 company objects with name, website, one_liner, long_description, stage, batch, tags, isHiring"
    - path: "tests/phase2/test_integration.py"
      provides: "End-to-end test: 5 fixture leads from Scout through Writer, all in review queue with no errors"
      exports: ["test_full_pipeline_e2e", "test_checkpoint_resume"]
  key_links:
    - from: "tests/phase2/conftest.py"
      to: "pydantic_ai.models.test.TestModel"
      via: "All PydanticAI agents are overridden with TestModel in test scope; no real LLM calls"
      pattern: "agent\\.override.*TestModel"
    - from: "tests/phase2/test_integration.py"
      to: "ingot.agents.orchestrator.run_pipeline"
      via: "E2E test calls run_pipeline() with fixture deps, verifies Email records created for all 5 leads"
      pattern: "run_pipeline.*fixture"
---

<objective>
Build the complete Phase 2 test suite — unit, integration, end-to-end, regression, and performance tests.

Purpose: Phase 2 is the v1 done condition. If the pipeline breaks, the product fails. This test suite provides the safety net: no test requires real API keys or network access, all LLM calls use PydanticAI's TestModel, and the performance benchmarks encode the targets (Scout <5s/100 companies, full pipeline <15s/5 leads) as enforceable assertions.
Output: `tests/phase2/` directory with conftest, all test files, fixture data, and coverage >= 70%.
</objective>

<execution_context>
@/Users/ishansingh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ishansingh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-RESEARCH.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-CONTEXT.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-06-SUMMARY.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-05-SUMMARY.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-04-SUMMARY.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-03-SUMMARY.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-02-SUMMARY.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-01-SUMMARY.md

<interfaces>
<!-- PydanticAI TestModel pattern (from 02-RESEARCH.md Code Examples) -->
```python
# Source: https://ai.pydantic.dev/testing/
from pydantic_ai.models.test import TestModel
from ingot.agents.profile import profile_agent

@pytest.fixture
def mock_profile_agent():
    with profile_agent.override(model=TestModel()):
        yield

# TestModel generates valid schema data automatically from output_type.
# result.output.name will be a non-None string (auto-generated).
# For deterministic values, use: TestModel(custom_result_args={'field': 'value'})
```

<!-- Fixture YC company structure (from 02-RESEARCH.md Pattern 2) -->
```json
{
  "id": 1, "name": "TestCo", "slug": "testco",
  "website": "https://testco.com",
  "one_liner": "Python SDK for API developers",
  "long_description": "We build Python and TypeScript tooling...",
  "team_size": 12, "industry": "Developer Tools",
  "tags": ["B2B", "Developer Tools"],
  "batch": "W25", "stage": "Seed",
  "isHiring": true, "status": "Active"
}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Conftest, fixtures, and unit tests (profile, scout, scoring)</name>
  <files>
    tests/phase2/__init__.py
    tests/phase2/conftest.py
    tests/phase2/fixtures/yc_companies_fixture.json
    tests/phase2/test_profile.py
    tests/phase2/test_scout.py
  </files>
  <action>
Build the shared test infrastructure and unit tests for profile and scout modules.

**tests/phase2/conftest.py** — all shared fixtures:

```python
"""
Phase 2 shared test fixtures.

KEY RULE: No real API calls. No real LLM calls.
  - PydanticAI agents: override with TestModel via agent.override()
  - httpx: use httpx.MockTransport or pytest-mock
  - SQLite: use temp directory, auto-cleaned between tests
  - YC data: use yc_companies_fixture.json (100 stable company records)
"""
import json
import tempfile
from pathlib import Path

import httpx
import pytest
import pytest_asyncio
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.asyncio import AsyncSession

from ingot.db.engine import create_engine, init_db
from ingot.db.models import Lead, LeadStatus
from ingot.models.schemas import (
    UserProfile, IntelBriefFull, MatchResult, MCQAnswers, EmailDraft
)
from datetime import datetime

FIXTURES_DIR = Path(__file__).parent / "fixtures"


@pytest_asyncio.fixture
async def fixture_db():
    """Temporary SQLite database, auto-cleaned after each test."""
    with tempfile.TemporaryDirectory() as tmpdir:
        engine = create_engine(f"sqlite+aiosqlite:///{tmpdir}/test.db")
        await init_db(engine)
        Session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
        async with Session() as session:
            yield session
        await engine.dispose()


@pytest.fixture
def fixture_user_profile() -> UserProfile:
    """Standard UserProfile for all pipeline tests."""
    return UserProfile(
        name="Jane Doe",
        headline="Senior Software Engineer",
        skills=["Python", "TypeScript", "React", "PostgreSQL", "REST APIs", "Docker"],
        experience=[
            "Senior Software Engineer at Stripe, 2021-2023",
            "Software Engineer at Twilio, 2019-2021",
        ],
        education=["BS Computer Science, UC Berkeley, 2019"],
        projects=["Built payment retry system handling 1M+ daily transactions"],
        github_url="https://github.com/janedoe",
        linkedin_url="https://linkedin.com/in/janedoe",
        resume_raw_text="Jane Doe\nSenior Software Engineer\nPython, TypeScript, React, PostgreSQL\nStripe 2021-2023, Twilio 2019-2021",
    )


@pytest.fixture
def fixture_intel_brief() -> IntelBriefFull:
    """Standard IntelBriefFull for writer and matcher tests."""
    return IntelBriefFull(
        company_name="DevTools Inc",
        company_signals=["Seed stage, 12 employees", "Recently launched Python SDK"],
        person_name="Alex Chen",
        person_role="CTO",
        company_website="https://devtools.io",
        person_background="Previously led engineering at Stripe; started DevTools Inc in 2024",
        talking_points=[
            "DevTools recently launched a Python SDK that competes in the API tooling space",
            "Alex Chen's background at Stripe aligns with Jane's Stripe experience",
            "Jane's payment retry system work maps directly to DevTools' reliability use cases",
        ],
        company_product_description="DevTools Inc builds a Python and TypeScript SDK for REST API development with built-in retry logic and observability.",
    )


@pytest.fixture
def fixture_match_result() -> MatchResult:
    return MatchResult(
        match_score=82.0,
        value_proposition="Jane's 3 years building Stripe's payment retry infrastructure maps directly to DevTools Inc's reliability-first SDK approach",
        confidence_level="high",
    )


@pytest_asyncio.fixture
async def fixture_leads(fixture_db) -> list[Lead]:
    """5 Lead records in SQLite, status='discovered'."""
    leads = []
    for i in range(5):
        lead = Lead(
            company_name=f"TestCompany{i}",
            company_website=f"https://testcompany{i}.com",
            person_name="",
            person_email="",
            status=LeadStatus.discovered,
            initial_score=0.7 - (i * 0.05),
            source_venue="yc-oss",
            created_at=datetime.utcnow(),
        )
        fixture_db.add(lead)
    await fixture_db.commit()
    # Reload all leads
    from sqlmodel import select
    result = await fixture_db.exec(select(Lead))
    leads = list(result.all())
    return leads


@pytest.fixture
def fixture_yc_companies() -> list[dict]:
    """100 YC company records from fixture file."""
    fixture_path = FIXTURES_DIR / "yc_companies_fixture.json"
    if not fixture_path.exists():
        # Generate minimal fixture if file missing
        companies = []
        stages = ["Seed", "Series A", "Series B", "Public"]
        for i in range(100):
            companies.append({
                "id": i + 1,
                "name": f"Company {i}",
                "slug": f"company-{i}",
                "website": f"https://company{i}.com",
                "one_liner": f"Python and TypeScript tools for developers at Company {i}",
                "long_description": f"Company {i} builds developer tooling with Python, TypeScript, and REST APIs",
                "team_size": 10 + i,
                "industry": "Developer Tools" if i % 3 == 0 else "SaaS",
                "tags": ["B2B", "Developer Tools"] if i % 2 == 0 else ["B2B", "SaaS"],
                "batch": "W25" if i < 50 else "S24",
                "stage": stages[i % 4],
                "isHiring": i % 3 == 0,
                "status": "Active",
            })
        fixture_path.parent.mkdir(parents=True, exist_ok=True)
        fixture_path.write_text(json.dumps(companies, indent=2))
    return json.loads(fixture_path.read_text())


@pytest.fixture
def mock_http_client(fixture_yc_companies):
    """Mock httpx.AsyncClient that returns fixture data for yc-oss URLs."""
    def handler(request: httpx.Request) -> httpx.Response:
        url = str(request.url)
        if "yc-oss.github.io" in url:
            return httpx.Response(200, json=fixture_yc_companies)
        # Company website fetch — return minimal HTML
        return httpx.Response(200, text="<html><body>Company info page</body></html>")

    transport = httpx.MockTransport(handler)
    return httpx.AsyncClient(transport=transport)
```

**tests/phase2/fixtures/yc_companies_fixture.json** — generate 100 company records:
This file will be auto-generated by conftest.py if it doesn't exist. Create it manually with 100 entries matching the schema above (name, website, one_liner, long_description, stage, batch, tags, isHiring, team_size, industry).

Create the JSON file with 100 company objects. Use this script to generate:
```python
import json
companies = []
stages = ["Seed", "Series A", "Series B", "Public"]
tech_terms = ["Python", "TypeScript", "React", "PostgreSQL", "Kubernetes", "GraphQL", "Rust"]
for i in range(100):
    t = tech_terms[i % len(tech_terms)]
    companies.append({
        "id": i + 1, "name": f"TechCo {i}", "slug": f"techco-{i}",
        "website": f"https://techco{i}.com",
        "one_liner": f"{t} tooling for modern API developers",
        "long_description": f"TechCo {i} builds {t} and developer infrastructure tools for REST API teams",
        "team_size": 5 + (i * 3), "industry": "Developer Tools",
        "tags": ["B2B", "Developer Tools"],
        "batch": "W25" if i < 50 else "S24",
        "stage": stages[i % 4], "isHiring": i % 3 == 0, "status": "Active"
    })
print(json.dumps(companies, indent=2))
```

**tests/phase2/test_profile.py:**

```python
"""
Tests for resume parsing and UserProfile extraction.
TEST-P2-02, TEST-P2-03
"""
import pytest
from ingot.agents.profile import (
    extract_pdf_text, extract_docx_text, parse_resume,
    validate_profile, profile_agent, ProfileDeps, ResumeParseError
)
from ingot.models.schemas import UserProfile
from pydantic_ai.models.test import TestModel


# TEST-P2-02: Resume parsing unit tests
class TestResumeParsing:
    def test_plain_text_fallback(self):
        """PROFILE-04: Plain text input works as fallback."""
        text = parse_resume(None, fallback_text="Jane Doe\nPython, React")
        assert "Jane Doe" in text
        assert "Python" in text

    def test_no_input_raises(self):
        """parse_resume raises ResumeParseError with no input."""
        with pytest.raises(ResumeParseError):
            parse_resume(None, None)

    def test_unsupported_format_raises(self, tmp_path):
        """Unsupported file extension raises ResumeParseError."""
        bad_file = tmp_path / "resume.txt"
        bad_file.write_text("Some text")
        with pytest.raises(ResumeParseError, match="Unsupported file type"):
            parse_resume(bad_file)


# TEST-P2-03: UserProfile extraction validation
class TestValidateProfile:
    def test_populated_profile_passes(self, fixture_user_profile):
        """Fully populated profile passes validation."""
        valid, reason = validate_profile(fixture_user_profile)
        assert valid, f"Expected valid: {reason}"

    def test_empty_profile_fails(self):
        """PROFILE-09: Empty profile (0/9 fields) fails validation."""
        empty = UserProfile(name="", resume_raw_text="")
        valid, reason = validate_profile(empty)
        assert not valid
        assert "0/" in reason or "retry" in reason.lower()

    def test_minimal_profile_passes(self):
        """Profile with name + resume_raw_text (2/9 = 22%) passes the 10% threshold."""
        minimal = UserProfile(name="Jane Doe", resume_raw_text="Jane Doe, Python developer")
        valid, reason = validate_profile(minimal)
        assert valid, f"Minimal profile should pass 10% threshold: {reason}"

    @pytest.mark.asyncio
    async def test_profile_agent_with_test_model(self):
        """TEST-P2-03: profile_agent runs with TestModel without real LLM call."""
        with profile_agent.override(model=TestModel()):
            result = await profile_agent.run(
                "Extract profile",
                deps=ProfileDeps(resume_text="Jane Doe\nPython, TypeScript"),
            )
            assert result.output is not None
            assert isinstance(result.output, UserProfile)
```

**tests/phase2/test_scout.py:**

```python
"""
Tests for Scout agent — YC fetch, scoring, deduplication.
TEST-P2-01, TEST-P2-07, TEST-P2-16 (performance)
"""
import time
import pytest
from ingot.scoring.scorer import ScoringWeights, score_lead, DEFAULT_WEIGHTS
from ingot.agents.scout import _validate_company_record, _is_duplicate, ScoutDeps, scout_run
from ingot.db.models import Lead, LeadStatus
from datetime import datetime


# TEST-P2-01: Lead deduplication
class TestLeadDeduplication:
    @pytest.mark.asyncio
    async def test_case_insensitive_email_dedup(self, fixture_db):
        """SCOUT-06: Same email with different case is detected as duplicate."""
        from ingot.db.models import LeadStatus
        lead = Lead(
            company_name="Acme", person_email="JANE@ACME.COM",
            status=LeadStatus.discovered, created_at=datetime.utcnow()
        )
        fixture_db.add(lead)
        await fixture_db.commit()

        # Check lowercase variant — should find the duplicate
        assert await _is_duplicate(fixture_db, "jane@acme.com")
        # Different email — should not find duplicate
        assert not await _is_duplicate(fixture_db, "other@acme.com")

    @pytest.mark.asyncio
    async def test_empty_email_not_deduped(self, fixture_db):
        """Empty person_email should not trigger dedup (allow through)."""
        assert not await _is_duplicate(fixture_db, "")
        assert not await _is_duplicate(fixture_db, None)


# TEST-P2-07 (partial) / TEST-P2-16 (performance): Scoring formula
class TestLeadScoring:
    def test_score_range_0_to_1(self, fixture_yc_companies):
        """All scored companies produce float 0.0-1.0."""
        for company in fixture_yc_companies[:20]:
            score = score_lead(company, ["Python", "TypeScript"])
            assert 0.0 <= score <= 1.0, f"Score out of range: {score} for {company['name']}"

    def test_weights_sum_to_one(self):
        """ScoringWeights components must sum to 1.0."""
        w = DEFAULT_WEIGHTS
        total = w.stack_domain_match + w.company_stage + w.job_keyword_match + w.semantic_similarity
        assert abs(total - 1.0) < 0.001

    def test_relevant_company_scores_higher(self, fixture_yc_companies):
        """Python/TypeScript developer tools company scores higher than unrelated."""
        relevant = next(
            c for c in fixture_yc_companies
            if "Python" in c.get("one_liner", "") and "Seed" in c.get("stage", "")
        )
        irrelevant = next(
            c for c in fixture_yc_companies
            if "Python" not in c.get("one_liner", "")
            and "Public" in c.get("stage", "")
        )
        rel_score = score_lead(relevant, ["Python", "TypeScript"])
        irr_score = score_lead(irrelevant, ["Python", "TypeScript"])
        assert rel_score > irr_score

    def test_validation_rejects_empty_name(self):
        """SCOUT-04: Company with empty name is rejected."""
        valid, reason = _validate_company_record({"name": "", "website": "https://example.com"})
        assert not valid

    def test_validation_accepts_complete_record(self):
        """SCOUT-04: Complete company record passes validation."""
        valid, _ = _validate_company_record({"name": "Acme", "website": "https://acme.com"})
        assert valid

    def test_performance_100_companies(self, fixture_yc_companies):
        """TEST-P2-16: score_lead on 100 companies completes in under 5 seconds."""
        start = time.time()
        for company in fixture_yc_companies:  # exactly 100
            score_lead(company, ["Python", "TypeScript", "React"], resume_text="Python developer")
        elapsed = time.time() - start
        assert elapsed < 5.0, f"Scoring 100 companies took {elapsed:.2f}s (limit: 5s)"
```
  </action>
  <verify>
    <automated>cd /Users/ishansingh/Desktop/job-hunter && python -m pytest tests/phase2/test_profile.py tests/phase2/test_scout.py -x -q 2>&1 | head -50</automated>
  </verify>
  <done>
    `tests/phase2/conftest.py` defines all fixtures. `yc_companies_fixture.json` contains 100 company records. `test_profile.py` tests pass (parsing, validation, TestModel). `test_scout.py` tests pass (dedup, scoring, validation, performance). No real LLM or network calls.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integration, e2e, regression, and performance tests</name>
  <files>
    tests/phase2/test_research.py
    tests/phase2/test_matcher.py
    tests/phase2/test_writer.py
    tests/phase2/test_orchestrator.py
    tests/phase2/test_integration.py
  </files>
  <action>
Build integration, e2e, regression, and performance tests for research, matcher, writer, and orchestrator.

**tests/phase2/test_research.py:**

```python
"""
Tests for Research agent — Phase 1, approval gate, Phase 2.
TEST-P2-08, TEST-P2-09, TEST-P2-10
"""
import pytest
from unittest.mock import patch, MagicMock
from pydantic_ai.models.test import TestModel
from ingot.agents.research import (
    research_agent_phase1, research_agent_phase2,
    ResearchDeps, research_phase1, research_phase2,
    run_approval_gate, ResearchError
)
from ingot.db.models import Lead, LeadStatus, IntelBrief
from ingot.models.schemas import IntelBriefPhase1, IntelBriefFull
from datetime import datetime
from sqlmodel import select


class TestResearchPhase1:
    @pytest.mark.asyncio
    async def test_phase1_with_test_model(self, fixture_db, mock_http_client):
        """TEST-P2-08: Phase 1 Research produces IntelBriefPhase1 with TestModel."""
        lead = Lead(
            company_name="TestCo", company_website="https://testco.com",
            status=LeadStatus.discovered, created_at=datetime.utcnow()
        )
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        with research_agent_phase1.override(model=TestModel()):
            deps = ResearchDeps(http_client=mock_http_client, session=fixture_db, lead=lead)
            phase1 = await research_phase1(deps)

        assert phase1 is not None
        assert isinstance(phase1, IntelBriefPhase1)
        # Verify IntelBrief was persisted
        result = await fixture_db.exec(select(IntelBrief).where(IntelBrief.lead_id == lead.id))
        brief_db = result.first()
        assert brief_db is not None
        assert brief_db.lead_id == lead.id

    @pytest.mark.asyncio
    async def test_phase1_sets_researching_status_before_llm(self, fixture_db, mock_http_client):
        """PITFALL-7: Lead status must be 'researching' before LLM call."""
        lead = Lead(
            company_name="StatusTest", company_website="https://statustest.com",
            status=LeadStatus.discovered, created_at=datetime.utcnow()
        )
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        status_during_call = []

        async def mock_run(prompt, deps, usage_limits=None):
            # Capture Lead status at time of LLM call
            await fixture_db.refresh(deps.lead)
            status_during_call.append(deps.lead.status)
            # Return mock result
            from pydantic_ai import RunResult
            return MagicMock(output=IntelBriefPhase1(
                company_name="StatusTest",
                company_signals=["Signal 1"],
            ))

        with patch.object(research_agent_phase1, 'run', side_effect=mock_run):
            deps = ResearchDeps(http_client=mock_http_client, session=fixture_db, lead=lead)
            try:
                await research_phase1(deps)
            except Exception:
                pass  # May fail due to mock, but we captured status

        # Whether or not it succeeded, the lead status should have been set to researching
        # before the LLM call (or the mock captured it in researching state)
        assert LeadStatus.researching in status_during_call or lead.status == LeadStatus.researching


class TestApprovalGate:
    def test_approval_gate_returns_valid_action(self, fixture_db):
        """TEST-P2-09: Approval gate returns accept/reject/defer."""
        lead = Lead(company_name="TestCo", status=LeadStatus.researching, created_at=datetime.utcnow())
        phase1 = IntelBriefPhase1(
            company_name="TestCo",
            company_signals=["Seed stage", "12 employees"],
        )
        with patch("questionary.select") as mock_select:
            mock_select.return_value.ask.return_value = "accept"
            action = run_approval_gate(lead, phase1)
        assert action == "accept"

    def test_approval_gate_handles_ctrl_c(self):
        """Ctrl+C (None from questionary) defaults to 'defer'."""
        lead = Lead(company_name="TestCo", status=LeadStatus.researching, created_at=datetime.utcnow())
        phase1 = IntelBriefPhase1(company_name="TestCo", company_signals=[])
        with patch("questionary.select") as mock_select:
            mock_select.return_value.ask.return_value = None  # Ctrl+C
            action = run_approval_gate(lead, phase1)
        assert action == "defer"


class TestResearchPhase2:
    @pytest.mark.asyncio
    async def test_phase2_rejected_lead_raises(self, fixture_db, mock_http_client):
        """Phase 2 must not run for rejected leads."""
        lead = Lead(
            company_name="Rejected", status=LeadStatus.rejected, created_at=datetime.utcnow()
        )
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        with pytest.raises(ResearchError, match="Phase 2 called"):
            deps = ResearchDeps(http_client=mock_http_client, session=fixture_db, lead=lead)
            await research_phase2(deps)

    @pytest.mark.asyncio
    async def test_phase2_with_test_model(self, fixture_db, mock_http_client):
        """TEST-P2-10: Phase 2 produces IntelBriefFull with at least 1 talking point."""
        lead = Lead(
            company_name="ApprovedCo", company_website="https://approved.com",
            status=LeadStatus.approved, created_at=datetime.utcnow()
        )
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        with research_agent_phase2.override(model=TestModel()):
            deps = ResearchDeps(http_client=mock_http_client, session=fixture_db, lead=lead)
            full_brief = await research_phase2(deps)

        assert isinstance(full_brief, IntelBriefFull)
        assert len(full_brief.talking_points) >= 1
```

**tests/phase2/test_matcher.py:**

```python
"""
Tests for Matcher agent.
TEST-P2-04, TEST-P2-11
"""
import pytest
from pydantic_ai.models.test import TestModel
from pydantic import ValidationError
from ingot.agents.matcher import MatcherDeps, matcher_agent, run_matcher
from ingot.models.schemas import MatchResult
from ingot.db.models import Lead, LeadStatus, Match
from sqlmodel import select
from datetime import datetime


class TestMatchScoring:
    def test_match_score_range_enforced(self):
        """TEST-P2-04: MatchResult rejects score outside 0-100."""
        with pytest.raises(ValidationError):
            MatchResult(match_score=150.0, value_proposition="x", confidence_level="high")
        with pytest.raises(ValidationError):
            MatchResult(match_score=-5.0, value_proposition="x", confidence_level="low")

    def test_valid_match_result(self):
        """Valid MatchResult instantiates correctly."""
        mr = MatchResult(match_score=75.0, value_proposition="Strong Python match", confidence_level="high")
        assert mr.match_score == 75.0
        assert mr.confidence_level == "high"

    @pytest.mark.asyncio
    async def test_matcher_with_test_model(self, fixture_db, fixture_user_profile, fixture_intel_brief):
        """TEST-P2-11: Matcher produces MatchResult and persists Match record."""
        lead = Lead(
            company_name="DevTools Inc", status=LeadStatus.approved, created_at=datetime.utcnow()
        )
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        with matcher_agent.override(model=TestModel()):
            deps = MatcherDeps(
                user_profile=fixture_user_profile,
                intel_brief=fixture_intel_brief,
                match_result=None,
                lead=lead,
                session=fixture_db,
            )
            result = await run_matcher(deps)

        assert isinstance(result, MatchResult)
        # Verify Match was persisted
        match_result_db = await fixture_db.exec(select(Match).where(Match.lead_id == lead.id))
        match_db = match_result_db.first()
        assert match_db is not None
        assert match_db.lead_id == lead.id
        # Verify Lead status updated
        await fixture_db.refresh(lead)
        assert lead.status == LeadStatus.matched
```

**tests/phase2/test_writer.py:**

```python
"""
Tests for Writer agent — MCQ, email generation, CAN-SPAM footer.
TEST-P2-05, TEST-P2-06, TEST-P2-12
"""
import pytest
from unittest.mock import patch
from pydantic_ai.models.test import TestModel
from ingot.agents.writer import (
    WriterDeps, writer_agent, mcq_agent, run_writer, run_mcq,
    build_can_spam_footer, _TONE_PROMPTS
)
from ingot.models.schemas import MCQAnswers, EmailDraft
from ingot.db.models import Lead, LeadStatus, Email, FollowUp
from sqlmodel import select
from datetime import datetime


class TestCANSPAMFooter:
    def test_footer_has_all_three_elements(self):
        """TEST-P2-06: CAN-SPAM footer must have sender, address, and unsubscribe."""
        footer = build_can_spam_footer(
            sender_name="Jane Doe",
            sender_email="jane@example.com",
            physical_address="123 Main St, SF, CA 94105",
        )
        assert "Jane Doe" in footer, "Missing sender identity"
        assert "123 Main St" in footer, "Missing physical address"
        assert "unsubscribe" in footer.lower(), "Missing unsubscribe mechanism"

    def test_footer_warns_on_empty_address(self):
        """Missing physical address warns but does not crash."""
        import warnings
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            footer = build_can_spam_footer("Jane", "jane@example.com", "")
            assert len(w) == 1
            assert "physical_address" in str(w[0].message).lower()
        assert "configure" in footer.lower()


class TestToneAdaptation:
    def test_tone_prompts_for_all_types(self):
        """TEST-P2-05: All 4 tone types are configured."""
        for tone in ["hr", "cto", "ceo", "default"]:
            assert tone in _TONE_PROMPTS
            assert len(_TONE_PROMPTS[tone]) > 50

    def test_hr_tone_mentions_credentials(self):
        """HR tone prompt emphasizes credentials/experience."""
        hr = _TONE_PROMPTS["hr"].lower()
        assert "credential" in hr or "experience" in hr or "highlight" in hr

    def test_cto_tone_mentions_brevity(self):
        """CTO tone prompt emphasizes brevity."""
        cto = _TONE_PROMPTS["cto"].lower()
        assert "short" in cto or "brief" in cto or "direct" in cto or "busy" in cto


class TestMCQFlow:
    @pytest.mark.asyncio
    async def test_mcq_skip_returns_skipped_answers(
        self, fixture_db, fixture_user_profile, fixture_intel_brief, fixture_match_result
    ):
        """TEST-P2-12: Skipped MCQ returns MCQAnswers(skipped=True)."""
        lead = Lead(company_name="TestCo", status=LeadStatus.matched, created_at=datetime.utcnow())
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        deps = WriterDeps(
            user_profile=fixture_user_profile,
            intel_brief=fixture_intel_brief,
            match_result=fixture_match_result,
            lead=lead,
            session=fixture_db,
        )
        with patch("questionary.confirm") as mock_confirm:
            mock_confirm.return_value.ask.return_value = False  # User skips
            result = await run_mcq(deps)

        assert result.skipped is True
        assert result.answers == {}

    @pytest.mark.asyncio
    async def test_writer_persists_email_and_followups(
        self, fixture_db, fixture_user_profile, fixture_intel_brief, fixture_match_result
    ):
        """TEST-P2-05: Writer persists Email + 2 FollowUp records."""
        lead = Lead(company_name="WriterTest", status=LeadStatus.matched, created_at=datetime.utcnow())
        fixture_db.add(lead)
        await fixture_db.commit()
        await fixture_db.refresh(lead)

        deps = WriterDeps(
            user_profile=fixture_user_profile,
            intel_brief=fixture_intel_brief,
            match_result=fixture_match_result,
            lead=lead,
            session=fixture_db,
            physical_address="123 Main St, SF, CA 94105",
            sender_name="Jane Doe",
            sender_email="jane@example.com",
            mcq_answers=MCQAnswers(skipped=True),
        )
        with writer_agent.override(model=TestModel()):
            await run_writer(deps)

        # Verify Email record
        email_result = await fixture_db.exec(select(Email).where(Email.lead_id == lead.id))
        email_db = email_result.first()
        assert email_db is not None

        # Verify 2 FollowUp records (day 3 and day 7)
        fu_result = await fixture_db.exec(
            select(FollowUp).where(FollowUp.parent_email_id == email_db.id)
        )
        followups = list(fu_result.all())
        days = {f.scheduled_for_day for f in followups}
        assert 3 in days, "Missing Day 3 follow-up"
        assert 7 in days, "Missing Day 7 follow-up"

        # Verify Lead status
        await fixture_db.refresh(lead)
        assert lead.status == LeadStatus.drafted
```

**tests/phase2/test_orchestrator.py:**

```python
"""
Tests for Orchestrator — checkpoint/resume, pipeline wiring.
TEST-P2-15
"""
import pytest
from unittest.mock import patch, AsyncMock
from ingot.agents.orchestrator import OrchestratorDeps, run_pipeline
from ingot.db.models import Lead, LeadStatus, Email
from sqlmodel import select
from datetime import datetime
import httpx


class TestCheckpointResume:
    @pytest.mark.asyncio
    async def test_no_duplicate_leads_on_resume(self, fixture_db, fixture_user_profile, mock_http_client):
        """TEST-P2-15: Pipeline resumption does not create duplicate Lead records."""
        # Pre-populate: 2 leads already in "approved" state (simulating mid-run crash)
        for i in range(2):
            lead = Lead(
                company_name=f"PreExisting {i}",
                company_website=f"https://pre{i}.com",
                status=LeadStatus.approved,
                created_at=datetime.utcnow(),
            )
            fixture_db.add(lead)
        await fixture_db.commit()

        initial_result = await fixture_db.exec(select(Lead))
        initial_count = len(list(initial_result.all()))

        # Run pipeline with mocked agents that do nothing
        with patch("ingot.agents.orchestrator.scout_run", new_callable=AsyncMock) as mock_scout:
            mock_scout.return_value = []  # Scout returns nothing — existing leads used

            with patch("ingot.agents.orchestrator.research_phase1", new_callable=AsyncMock):
                with patch("ingot.agents.orchestrator.research_phase2", new_callable=AsyncMock):
                    with patch("ingot.agents.orchestrator.run_matcher", new_callable=AsyncMock):
                        with patch("ingot.agents.orchestrator.run_writer", new_callable=AsyncMock):
                            with patch("ingot.agents.orchestrator.run_review_queue", new_callable=AsyncMock) as mock_queue:
                                mock_queue.return_value = {}
                                deps = OrchestratorDeps(
                                    session=fixture_db,
                                    http_client=mock_http_client,
                                    user_profile=fixture_user_profile,
                                    user_skills=fixture_user_profile.skills,
                                    resume_text=fixture_user_profile.resume_raw_text,
                                )
                                await run_pipeline(deps)

        # Verify no new leads were duplicated
        final_result = await fixture_db.exec(select(Lead))
        final_count = len(list(final_result.all()))
        assert final_count == initial_count, f"Leads duplicated: {initial_count} -> {final_count}"
```

**tests/phase2/test_integration.py:**

```python
"""
End-to-end and integration tests.
TEST-P2-13, TEST-P2-14, TEST-P2-15, TEST-P2-16
"""
import time
import pytest
from unittest.mock import patch, AsyncMock
from pydantic_ai.models.test import TestModel
from ingot.agents.orchestrator import OrchestratorDeps, run_pipeline
from ingot.agents.profile import profile_agent, ProfileDeps, validate_profile
from ingot.agents.research import research_agent_phase1, research_agent_phase2
from ingot.agents.matcher import matcher_agent
from ingot.agents.writer import writer_agent, mcq_agent
from ingot.db.models import Lead, Email, LeadStatus
from ingot.models.schemas import UserProfile
from sqlmodel import select
from datetime import datetime
import httpx


@pytest.fixture
def mock_approval_gate_accept():
    """Always accept in the approval gate for integration tests."""
    with patch("ingot.agents.orchestrator.run_approval_gate", return_value="accept"):
        yield


@pytest.fixture
def mock_mcq_skip():
    """Always skip MCQ for integration tests."""
    with patch("ingot.agents.orchestrator.run_mcq", new_callable=AsyncMock) as mock:
        from ingot.models.schemas import MCQAnswers
        mock.return_value = MCQAnswers(skipped=True)
        yield


@pytest.fixture
def mock_review_queue():
    """Auto-approve all leads in review queue for integration tests."""
    with patch("ingot.agents.orchestrator.run_review_queue", new_callable=AsyncMock) as mock:
        mock.return_value = {}
        yield


class TestFullPipelineE2E:
    @pytest.mark.asyncio
    async def test_full_pipeline_5_leads(
        self, fixture_db, fixture_user_profile, mock_http_client,
        mock_approval_gate_accept, mock_mcq_skip, mock_review_queue,
        fixture_yc_companies
    ):
        """
        TEST-P2-13: Full pipeline on 5 fixture leads completes without unhandled errors.
        All fixture companies are returned by mock_http_client.
        All agents use TestModel.
        """
        with (
            research_agent_phase1.override(model=TestModel()),
            research_agent_phase2.override(model=TestModel()),
            matcher_agent.override(model=TestModel()),
            writer_agent.override(model=TestModel()),
            mcq_agent.override(model=TestModel()),
        ):
            with patch("ingot.agents.orchestrator.run_review_queue", new_callable=AsyncMock) as mock_rq:
                mock_rq.return_value = {}
                with patch("questionary.confirm") as mock_confirm:
                    mock_confirm.return_value.ask.return_value = False  # Skip MCQ

                    deps = OrchestratorDeps(
                        session=fixture_db,
                        http_client=mock_http_client,
                        user_profile=fixture_user_profile,
                        user_skills=fixture_user_profile.skills,
                        resume_text=fixture_user_profile.resume_raw_text,
                        sender_name="Jane Doe",
                        sender_email="jane@example.com",
                        physical_address="123 Main St, SF, CA 94105",
                    )
                    await run_pipeline(deps)

        # Verify leads were created
        result = await fixture_db.exec(select(Lead))
        leads = list(result.all())
        assert len(leads) > 0, "No leads were created"

    def test_performance_scoring_100_companies(self, fixture_yc_companies):
        """TEST-P2-16: score_lead on 100 YC fixture companies completes in <5s."""
        from ingot.scoring.scorer import score_lead
        start = time.time()
        for company in fixture_yc_companies:
            score_lead(company, ["Python", "TypeScript", "React"])
        elapsed = time.time() - start
        assert elapsed < 5.0, f"Scoring 100 companies took {elapsed:.2f}s (limit: 5s)"
```
  </action>
  <verify>
    <automated>cd /Users/ishansingh/Desktop/job-hunter && python -m pytest tests/phase2/ -x -q --tb=short 2>&1 | tail -30</automated>
  </verify>
  <done>
    All test files in `tests/phase2/` are created. `pytest tests/phase2/ -x -q` runs without collection errors. Unit tests for profile, scout, research, matcher, and writer all pass. Integration and e2e tests pass. Performance benchmark for 100-company scoring asserts under 5 seconds.
  </done>
</task>

</tasks>

<verification>
Run after all tasks complete:

```bash
# Full test suite with coverage
cd /Users/ishansingh/Desktop/job-hunter
python -m pytest tests/phase2/ -v --tb=short 2>&1 | tail -50

# Coverage report for phase 2 modules
python -m pytest tests/phase2/ --cov=ingot.agents --cov=ingot.venues --cov=ingot.scoring --cov=ingot.review --cov-report=term-missing 2>&1 | grep -E "TOTAL|agents|venues|scoring|review"

# Performance benchmark specifically
python -m pytest tests/phase2/test_integration.py::TestFullPipelineE2E::test_performance_scoring_100_companies -v

# Verify no real network calls in test suite (all imports must work without network)
python -c "
import sys
# Block network to confirm no real calls
import socket
original_connect = socket.socket.connect
def mock_connect(self, *args):
    raise ConnectionRefusedError('No network in test mode')
socket.socket.connect = mock_connect

# These should all import successfully (no network at import time)
from ingot.agents.profile import profile_agent
from ingot.agents.scout import scout_run
from ingot.agents.research import research_agent_phase1
from ingot.agents.matcher import matcher_agent
from ingot.agents.writer import writer_agent
print('All agents import without network access OK')
socket.socket.connect = original_connect
"
```
</verification>

<success_criteria>
- `pytest tests/phase2/ -x -q` exits 0 — all tests pass
- Coverage on `ingot.agents.*`, `ingot.scoring.*`, `ingot.venues.*`, `ingot.review.*` >= 70%
- Performance: `score_lead()` on 100 companies < 5 seconds (TEST-P2-16)
- No test requires real API keys, real YC network, or real LLM calls
- `fixture_yc_companies` fixture provides 100 stable company records
- Checkpoint/resume test: no duplicate Lead records on pipeline resumption (TEST-P2-15)
- CAN-SPAM footer test: all 3 mandatory elements validated (TEST-P2-06)
- TestModel used for all PydanticAI agents via `agent.override()`
- TEST-P2-01 through TEST-P2-16 requirements all addressed
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-pipeline-scout-through-writer/02-07-SUMMARY.md` with:
- Final test count (number of tests collected and passed)
- Coverage percentage for each phase 2 module
- Performance benchmark results (scoring time for 100 companies)
- Any tests that were skipped or xfailed and why
- TestModel behavior notes (what default values it generates for output schemas)
</output>
