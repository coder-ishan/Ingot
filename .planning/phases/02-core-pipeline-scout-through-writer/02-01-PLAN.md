---
phase: 02-core-pipeline-scout-through-writer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ingot/agents/profile.py
  - src/ingot/models/schemas.py
  - src/ingot/models/__init__.py
  - pyproject.toml
autonomous: true
requirements:
  - PROFILE-01
  - PROFILE-02
  - PROFILE-03
  - PROFILE-04
  - PROFILE-05
  - PROFILE-06
  - PROFILE-07
  - PROFILE-08
  - PROFILE-09

must_haves:
  truths:
    - "PDF resume extraction uses column_boxes() for multi-column layout detection — single-column PDF and two-column PDF both produce coherent text (skills section is not interleaved inside experience entries)"
    - "DOCX resume extraction uses iter_inner_content() to preserve paragraph/table interleave order"
    - "If PDF and DOCX parsing both fail, user is prompted to paste plain text — plain-text path feeds into the same LLM extraction step"
    - "LLM extraction returns a UserProfile Pydantic model with all required fields; validation failure raises a typed error with context"
    - "If fewer than 10% of the 9 UserProfile fields (name, headline, skills, experience, education, projects, github_url, linkedin_url, resume_raw_text) are populated, extraction is rejected and user is prompted to retry with raw text"
    - "UserProfile is persisted to SQLite and can be reloaded by Matcher and Writer agents via the db session"
  artifacts:
    - path: "src/ingot/models/schemas.py"
      provides: "UserProfile, IntelBriefPhase1, IntelBriefFull, MatchResult, EmailDraft, MCQAnswers Pydantic output schemas for all Phase 2 agents"
      exports: ["UserProfile", "IntelBriefPhase1", "IntelBriefFull", "MatchResult", "EmailDraft", "MCQAnswers"]
    - path: "src/ingot/agents/profile.py"
      provides: "resume_to_text() parser, ProfileDeps dataclass, profile_agent (PydanticAI), extract_profile() async function, validate_profile() function"
      exports: ["extract_profile", "validate_profile", "ProfileDeps", "profile_agent"]
  key_links:
    - from: "src/ingot/agents/profile.py"
      to: "src/ingot/models/schemas.py"
      via: "profile_agent uses output_type=UserProfile from schemas"
      pattern: "output_type=UserProfile"
    - from: "src/ingot/agents/profile.py"
      to: "src/ingot/db/models.py"
      via: "extract_profile() persists UserProfile to SQLite via AsyncSession"
      pattern: "session.add.*UserProfile"
---

<objective>
Build the resume parsing pipeline and UserProfile extraction agent — the foundation for Matcher and Writer personalization.

Purpose: Every downstream agent (Matcher, Writer) depends on a structured UserProfile loaded from the user's resume. Without this, personalization is impossible — no skills to match, no experience to reference, no talking points to ground the email.
Output: `src/ingot/models/schemas.py` (all Phase 2 Pydantic output schemas), `src/ingot/agents/profile.py` (parser + PydanticAI extraction agent).
</objective>

<execution_context>
@/Users/ishansingh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ishansingh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-RESEARCH.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-CONTEXT.md
@src/ingot/config/manager.py
@src/ingot/db/models.py

<interfaces>
<!-- Phase 1 services the executor will import -->

From src/ingot/config/manager.py:
```python
class ConfigManager:
    def __init__(self, base_dir: Path | None = None) -> None: ...
    def load(self) -> AppConfig: ...
    def get_db_path(self) -> Path: ...
```

From src/ingot/db/models.py:
```python
class UserProfile(SQLModel, table=True):
    id: int | None = Field(default=None, primary_key=True)
    name: str
    headline: str
    skills: list[str]         # JSON column
    experience: list[dict]    # JSON column
    education: list[dict]     # JSON column
    projects: list[dict]      # JSON column
    github_url: str
    linkedin_url: str
    resume_raw_text: str
    created_at: datetime
    updated_at: datetime

# AsyncSession from src/ingot/db/engine.py:
async def get_session() -> AsyncGenerator[AsyncSession, None]: ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Phase 2 Pydantic output schemas (central contracts file)</name>
  <files>
    src/ingot/models/__init__.py
    src/ingot/models/schemas.py
  </files>
  <action>
Create `src/ingot/models/schemas.py` as the single source of all PydanticAI `output_type` schemas for Phase 2. This file defines contracts — all agents import from here; nothing imports from agents.

**src/ingot/models/schemas.py** — implement these Pydantic models (use `from pydantic import BaseModel, Field, field_validator`):

```python
class UserProfile(BaseModel):
    """Extracted from resume. Injected into Matcher and Writer as deps."""
    name: str
    headline: str = ""
    skills: list[str] = Field(default_factory=list)
    experience: list[str] = Field(default_factory=list)   # free-text entries, e.g. "Senior SWE at Stripe 2021-2023"
    education: list[str] = Field(default_factory=list)
    projects: list[str] = Field(default_factory=list)
    github_url: str | None = None
    linkedin_url: str | None = None
    resume_raw_text: str = ""

class IntelBriefPhase1(BaseModel):
    """Phase 1 Research output — lightweight, produced before approval gate."""
    company_name: str
    company_signals: list[str] = Field(default_factory=list)  # funding stage, size, growth signals
    person_name: str = ""
    person_role: str = ""
    company_website: str = ""

class IntelBriefFull(BaseModel):
    """Phase 2 Research output — full intel with talking points and person background."""
    company_name: str
    company_signals: list[str] = Field(default_factory=list)
    person_name: str = ""
    person_role: str = ""
    company_website: str = ""
    person_background: str = ""
    talking_points: list[str] = Field(default_factory=list, min_length=1, max_length=3)
    company_product_description: str = ""

    @field_validator("talking_points")
    @classmethod
    def at_least_one_talking_point(cls, v: list[str]) -> list[str]:
        if not v:
            raise ValueError("IntelBriefFull must have at least one talking point")
        return v

class MatchResult(BaseModel):
    """Matcher agent output."""
    match_score: float = Field(ge=0.0, le=100.0)
    value_proposition: str   # specific to this company/role, not generic
    confidence_level: str    # "high" | "medium" | "low"

class MCQAnswers(BaseModel):
    """MCQ flow answers passed to Writer."""
    answers: dict[str, str] = Field(default_factory=dict)  # question -> answer
    skipped: bool = False

class EmailDraft(BaseModel):
    """Writer agent output — full draft set for one lead."""
    subject_a: str
    subject_b: str
    body: str
    tone_adapted_for: str    # "hr" | "cto" | "ceo" | "default"
    followup_day3: str
    followup_day7: str
    can_spam_footer: str

    @field_validator("body")
    @classmethod
    def body_must_mention_company(cls, v: str, info) -> str:
        # Validated post-generation: body must reference something specific
        # This is a soft check — passes unless body is suspiciously short
        if len(v) < 100:
            raise ValueError("Email body is too short to be personalized (< 100 chars)")
        return v
```

**src/ingot/models/__init__.py** — export all schemas:
```python
from ingot.models.schemas import (
    UserProfile, IntelBriefPhase1, IntelBriefFull,
    MatchResult, MCQAnswers, EmailDraft
)
__all__ = ["UserProfile", "IntelBriefPhase1", "IntelBriefFull", "MatchResult", "MCQAnswers", "EmailDraft"]
```

CRITICAL NOTE: These are **Pydantic BaseModel** schemas (agent I/O), NOT the SQLModel table models in `src/ingot/db/models.py`. The SQLModel `UserProfile` table is the persistence layer; this `UserProfile` BaseModel is the LLM extraction contract. They have different import paths. The `profile_agent` in Task 2 maps the BaseModel output into the SQLModel table for persistence.
  </action>
  <verify>
    <automated>python -c "
from ingot.models.schemas import UserProfile, IntelBriefPhase1, IntelBriefFull, MatchResult, MCQAnswers, EmailDraft
# Validate instantiation with defaults
up = UserProfile(name='Jane Doe', resume_raw_text='Jane Doe Python React')
assert up.name == 'Jane Doe'
assert up.skills == []
ib = IntelBriefFull(company_name='Acme', talking_points=['We ship fast'])
assert len(ib.talking_points) == 1
mr = MatchResult(match_score=85.0, value_proposition='Strong Python backend fit', confidence_level='high')
assert 0 <= mr.match_score <= 100
ed = EmailDraft(subject_a='Re: Acme', subject_b='Quick question', body='Hi Jane, I came across Acme and was impressed by your approach to developer tooling. My 3 years at Stripe building payment APIs maps directly to your infra challenges. Would love to connect.', tone_adapted_for='cto', followup_day3='Following up...', followup_day7='Last nudge...', can_spam_footer='Unsubscribe | 123 Main St')
print('All schemas OK')
"
    </automated>
  </verify>
  <done>
    All 6 schema classes import from `ingot.models.schemas` and instantiate without error. `IntelBriefFull` raises `ValueError` if `talking_points` is empty. `EmailDraft` raises `ValueError` if `body` is under 100 chars. `MatchResult` enforces `match_score` 0-100 range.
  </done>
</task>

<task type="auto">
  <name>Task 2: Resume parser and profile extraction agent</name>
  <files>
    src/ingot/agents/profile.py
    src/ingot/agents/__init__.py
    pyproject.toml
  </files>
  <action>
Build the resume parsing pipeline (PDF, DOCX, plain-text fallback) and the PydanticAI extraction agent.

**Add missing dependencies to pyproject.toml** (under `[project] dependencies`):
```
"PyMuPDF>=1.24",
"python-docx>=1.1",
"beautifulsoup4>=4.12",
"scikit-learn>=1.5",
"lxml>=5.0",
```

**src/ingot/agents/profile.py** — implement in this order:

**1. PDF parser (PROFILE-02) — multi-column aware:**
```python
def extract_pdf_text(path: str | Path) -> str:
    """
    Extract text from PDF with multi-column layout support.

    CRITICAL: Do NOT use page.get_text(sort=True) alone — it interleaves columns.
    Use column_boxes() to detect column Rects, then extract per-column.
    Falls back to single-column get_text() if column_boxes returns nothing.
    """
    import pymupdf
    # column_boxes may be in pymupdf.utils or pymupdf directly depending on version.
    # Try import paths in order; if neither works, copy multi_column.py from PyMuPDF-Utilities.
    try:
        from pymupdf import column_boxes
    except ImportError:
        try:
            from pymupdf.utils import column_boxes
        except ImportError:
            column_boxes = None  # fallback to single-column

    doc = pymupdf.open(str(path))
    full_text: list[str] = []
    for page in doc:
        if column_boxes is not None:
            cols = column_boxes(page, footer_margin=50, no_image_text=True)
        else:
            cols = []
        if cols:
            for col_rect in cols:
                col_text = page.get_text(clip=col_rect, sort=True)
                full_text.append(col_text.strip())
        else:
            full_text.append(page.get_text(sort=True).strip())
    doc.close()
    return "\n\n".join(t for t in full_text if t)
```

**2. DOCX parser (PROFILE-03):**
```python
def extract_docx_text(path: str | Path) -> str:
    """Extract text from DOCX preserving paragraph/table interleave order."""
    from docx import Document
    doc = Document(str(path))
    parts: list[str] = []
    for item in doc.element.body.iter_inner_content():
        # Paragraphs have .text; tables need row iteration
        if hasattr(item, 'text') and item.text.strip():
            parts.append(item.text.strip())
        elif hasattr(item, 'rows'):
            for row in item.rows:
                row_text = " | ".join(cell.text.strip() for cell in row.cells if cell.text.strip())
                if row_text:
                    parts.append(row_text)
    return "\n".join(parts)
```

**3. Main parser dispatcher (PROFILE-01, PROFILE-04):**
```python
def parse_resume(path: str | Path | None, fallback_text: str | None = None) -> str:
    """
    Parse resume from file or fall back to plain text.
    Returns raw text ready for LLM extraction.
    Raises ResumeParseError if no input is provided.
    """
    if path is not None:
        path = Path(path)
        if path.suffix.lower() == ".pdf":
            try:
                return extract_pdf_text(path)
            except Exception as e:
                raise ResumeParseError(f"PDF parsing failed: {e}") from e
        elif path.suffix.lower() in (".docx", ".doc"):
            try:
                return extract_docx_text(path)
            except Exception as e:
                raise ResumeParseError(f"DOCX parsing failed: {e}") from e
        else:
            raise ResumeParseError(f"Unsupported file type: {path.suffix}. Use PDF or DOCX.")
    if fallback_text:
        return fallback_text
    raise ResumeParseError("No resume file or fallback text provided.")


class ResumeParseError(Exception):
    pass
```

**4. PydanticAI extraction agent (PROFILE-05, PROFILE-06):**
```python
from dataclasses import dataclass
from pydantic_ai import Agent, RunContext
from ingot.models.schemas import UserProfile

@dataclass
class ProfileDeps:
    resume_text: str

profile_agent = Agent(
    "anthropic:claude-3-5-haiku-latest",  # overridden per config in production
    deps_type=ProfileDeps,
    output_type=UserProfile,
    system_prompt=(
        "Extract a structured UserProfile from the resume text provided in your context. "
        "skills must be specific technologies and tools only (Python, React, PostgreSQL) — "
        "not soft skills (leadership, communication). "
        "experience entries should be concise: 'Role at Company, Year-Year'. "
        "If a field cannot be determined, return null for optional fields (github_url, linkedin_url). "
        "resume_raw_text must contain the full raw text passed to you."
    ),
)

@profile_agent.system_prompt
async def inject_resume(ctx: RunContext[ProfileDeps]) -> str:
    return f"\n\nRESUME TEXT:\n{ctx.deps.resume_text}"
```

**5. Orchestration function with validation (PROFILE-07, PROFILE-09):**
```python
from sqlalchemy.ext.asyncio import AsyncSession
import ingot.db.models as db_models

def validate_profile(profile: UserProfile) -> tuple[bool, str]:
    """
    PROFILE-09: Reject if < 10% of the 9 fields are meaningfully populated.
    Returns (is_valid, reason).
    """
    fields = [
        profile.name, profile.headline,
        profile.skills, profile.experience, profile.education,
        profile.projects, profile.github_url, profile.linkedin_url,
        profile.resume_raw_text,
    ]
    populated = sum(
        1 for f in fields
        if f is not None and (isinstance(f, list) and len(f) > 0 or isinstance(f, str) and f.strip())
    )
    threshold = max(1, int(len(fields) * 0.10))  # 10% of 9 = at least 1
    if populated < threshold:
        return False, f"Only {populated}/{len(fields)} fields extracted. Retry with plain text."
    return True, ""


async def extract_profile(
    resume_text: str,
    session: AsyncSession,
    model_override: str | None = None,
) -> db_models.UserProfile:
    """
    Run profile_agent to extract UserProfile, validate, and persist to SQLite.
    Returns the persisted SQLModel UserProfile record.

    PROFILE-08: Matcher and Writer load this record on every run.
    """
    from datetime import datetime

    agent = profile_agent
    result = await agent.run(
        "Extract the UserProfile from the resume text in your system prompt.",
        deps=ProfileDeps(resume_text=resume_text),
    )
    profile_schema: UserProfile = result.output

    is_valid, reason = validate_profile(profile_schema)
    if not is_valid:
        raise ResumeParseError(f"Extraction rejected: {reason}")

    # Map Pydantic schema -> SQLModel table row
    db_profile = db_models.UserProfile(
        name=profile_schema.name,
        headline=profile_schema.headline or "",
        skills=profile_schema.skills,
        experience=[{"entry": e} for e in profile_schema.experience],
        education=[{"entry": e} for e in profile_schema.education],
        projects=[{"entry": p} for p in profile_schema.projects],
        github_url=profile_schema.github_url or "",
        linkedin_url=profile_schema.linkedin_url or "",
        resume_raw_text=profile_schema.resume_raw_text or resume_text,
        created_at=datetime.utcnow(),
        updated_at=datetime.utcnow(),
    )
    session.add(db_profile)
    await session.commit()
    await session.refresh(db_profile)
    return db_profile
```

**src/ingot/agents/__init__.py** — create as empty package file if it doesn't exist.

The `profile_agent` model string `"anthropic:claude-3-5-haiku-latest"` is the default. In production it is overridden by reading `ConfigManager().load().agents["profile"].model` and passing it via `Agent(..., model=config_model)`. This wiring happens in Plan 02-06 (Orchestrator). For now the default is correct.
  </action>
  <verify>
    <automated>python -c "
import asyncio, tempfile, pathlib
from ingot.agents.profile import extract_pdf_text, extract_docx_text, parse_resume, validate_profile, ResumeParseError
from ingot.models.schemas import UserProfile

# Test validate_profile with populated profile
profile = UserProfile(
    name='Jane Doe',
    headline='Senior Software Engineer',
    skills=['Python', 'React'],
    resume_raw_text='Jane Doe\nPython, React\nStripe 2021-2023',
)
valid, reason = validate_profile(profile)
assert valid, f'Expected valid profile: {reason}'

# Test validate_profile with empty profile
empty_profile = UserProfile(name='', resume_raw_text='')
valid2, reason2 = validate_profile(empty_profile)
assert not valid2, 'Expected empty profile to fail validation'

# Test plain text fallback
text = parse_resume(None, fallback_text='Jane Doe, Python developer')
assert 'Jane Doe' in text

# Test no input raises
try:
    parse_resume(None, None)
    assert False, 'Should have raised ResumeParseError'
except ResumeParseError:
    pass

print('profile.py unit checks OK')
"
    </automated>
  </verify>
  <done>
    `parse_resume()` returns text for PDF/DOCX/plain-text inputs and raises `ResumeParseError` when given no input. `validate_profile()` returns `(False, reason)` when fewer than 10% of fields are populated and `(True, "")` for a populated profile. `extract_pdf_text()` and `extract_docx_text()` import without error. `profile_agent` is importable. `extract_profile()` is defined and imports `AsyncSession` and the db models.
  </done>
</task>

</tasks>

<verification>
Run after all tasks complete:

```bash
# Verify all schemas importable and valid
python -c "
from ingot.models import UserProfile, IntelBriefPhase1, IntelBriefFull, MatchResult, MCQAnswers, EmailDraft
from ingot.agents.profile import extract_pdf_text, extract_docx_text, parse_resume, validate_profile, extract_profile, profile_agent, ProfileDeps, ResumeParseError
print('All imports OK')

# Verify talking_points validator
try:
    IntelBriefFull(company_name='X', talking_points=[])
    print('ERROR: should have raised')
except Exception as e:
    print(f'talking_points validator OK: {e}')

# Verify body length validator
try:
    from ingot.models.schemas import EmailDraft
    EmailDraft(subject_a='A', subject_b='B', body='short', tone_adapted_for='cto', followup_day3='f', followup_day7='f', can_spam_footer='footer')
    print('ERROR: should have raised')
except Exception as e:
    print(f'body length validator OK: {e}')
"

# Verify pyproject.toml has new deps
python -c "
import tomllib
with open('pyproject.toml', 'rb') as f:
    data = tomllib.load(f)
deps = data['project']['dependencies']
required = ['PyMuPDF', 'python-docx', 'beautifulsoup4', 'scikit-learn']
for r in required:
    assert any(r.lower() in d.lower() for d in deps), f'Missing dep: {r}'
print('pyproject.toml deps OK')
"
```
</verification>

<success_criteria>
- All 6 Pydantic schemas (`UserProfile`, `IntelBriefPhase1`, `IntelBriefFull`, `MatchResult`, `MCQAnswers`, `EmailDraft`) importable from `ingot.models.schemas`
- `validate_profile()` rejects UserProfile with 0/9 populated fields, accepts profile with 3+ fields
- `parse_resume()` handles PDF path, DOCX path, plain-text fallback, and raises `ResumeParseError` for no input
- `profile_agent` is importable and configured with `output_type=UserProfile`, `deps_type=ProfileDeps`
- `extract_profile()` is async and maps UserProfile schema to SQLModel db record
- PyMuPDF, python-docx, beautifulsoup4, scikit-learn added to pyproject.toml
- PROFILE-01 through PROFILE-09 requirements all addressed
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-pipeline-scout-through-writer/02-01-SUMMARY.md` with:
- Exact schema field names (any deviations from REQUIREMENTS.md)
- column_boxes import path that worked (pymupdf vs pymupdf.utils vs utility copy)
- validate_profile threshold implementation (current: 10% of 9 fields = at least 1 populated)
- profile_agent system_prompt text (for Writer agent to use similar extraction pattern)
- New dependencies added to pyproject.toml
</output>
