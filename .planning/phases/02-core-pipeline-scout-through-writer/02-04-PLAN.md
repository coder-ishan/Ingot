---
phase: 02-core-pipeline-scout-through-writer
plan: 04
type: execute
wave: 2
depends_on:
  - "02-01"
  - "02-03"
files_modified:
  - src/ingot/agents/matcher.py
  - src/ingot/agents/__init__.py
autonomous: true
requirements:
  - MATCH-01
  - MATCH-02
  - MATCH-03
  - MATCH-04
  - MATCH-05

must_haves:
  truths:
    - "match_score is a float 0-100 (not 0.0-1.0) — Pydantic validator enforces the range"
    - "value_proposition is specific to the company and role — it references the IntelBrief's company_name, person_role, or talking_points (not a generic statement like 'I am a strong fit')"
    - "confidence_level is one of 'high' | 'medium' | 'low'"
    - "MatchResult is persisted to SQLite as a Match record linked to the Lead via lead_id"
    - "Lead.status transitions to 'matched' after successful Matcher run"
    - "matcher_agent receives UserProfile and IntelBriefFull via dependency injection — it does NOT query the database directly"
  artifacts:
    - path: "src/ingot/agents/matcher.py"
      provides: "MatcherDeps dataclass, matcher_agent (PydanticAI), run_matcher() async function"
      exports: ["MatcherDeps", "matcher_agent", "run_matcher"]
  key_links:
    - from: "src/ingot/agents/matcher.py"
      to: "src/ingot/models/schemas.py"
      via: "matcher_agent uses output_type=MatchResult"
      pattern: "output_type=MatchResult"
    - from: "src/ingot/agents/matcher.py"
      to: "src/ingot/db/models.py"
      via: "run_matcher() persists Match record with lead_id; updates Lead.status='matched'"
      pattern: "Match.*lead_id"
    - from: "MatcherDeps"
      to: "ingot.models.schemas.UserProfile"
      via: "deps.user_profile injected from SQLite UserProfile record loaded by Orchestrator"
      pattern: "user_profile.*UserProfile"
---

<objective>
Build the Matcher agent — match score calculation and value proposition generation.

Purpose: The Matcher takes the structured IntelBriefFull and the user's qualifications (UserProfile) and produces a 0-100 match score plus a specific value proposition for each lead. This data feeds directly into the Writer's email generation context — a vague value prop produces a generic email.
Output: `src/ingot/agents/matcher.py` with PydanticAI agent, MatcherDeps, and run_matcher() orchestration.
</objective>

<execution_context>
@/Users/ishansingh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ishansingh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-RESEARCH.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-CONTEXT.md
@src/ingot/db/models.py
@.planning/phases/02-core-pipeline-scout-through-writer/02-01-SUMMARY.md
@.planning/phases/02-core-pipeline-scout-through-writer/02-03-SUMMARY.md

<interfaces>
<!-- Schemas from 02-01 -->
From src/ingot/models/schemas.py:
```python
class UserProfile(BaseModel):
    name: str
    headline: str
    skills: list[str]
    experience: list[str]
    education: list[str]
    projects: list[str]
    github_url: str | None
    linkedin_url: str | None
    resume_raw_text: str

class IntelBriefFull(BaseModel):
    company_name: str
    company_signals: list[str]
    person_name: str
    person_role: str
    company_website: str
    person_background: str
    talking_points: list[str]    # at least 1 guaranteed by validator
    company_product_description: str

class MatchResult(BaseModel):
    match_score: float           # 0-100 range enforced by validator
    value_proposition: str       # specific, not generic
    confidence_level: str        # "high" | "medium" | "low"
```

<!-- SQLModel tables -->
From src/ingot/db/models.py:
```python
class Match(SQLModel, table=True):
    id: int | None
    match_score: float
    value_proposition: str
    confidence_level: str
    lead_id: int | None = Field(foreign_key="lead.id")
    created_at: datetime

class Lead(SQLModel, table=True):
    status: LeadStatus  # "approved" -> "matched" after Matcher runs
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Matcher agent — match score, value prop, confidence level</name>
  <files>
    src/ingot/agents/matcher.py
    src/ingot/agents/__init__.py
  </files>
  <action>
Build the Matcher agent. This is a PydanticAI agent that receives UserProfile + IntelBriefFull via deps and outputs a MatchResult.

The Matcher does NOT make tool calls — it is a pure reasoning agent (no httpx fetches). The system prompt includes the full scoring rubric so the LLM produces calibrated scores, not arbitrary numbers.

**src/ingot/agents/matcher.py:**

```python
"""
Matcher Agent — qualification matching and value proposition generation.

Input (via MatcherDeps):
  - UserProfile: user's skills, experience, and resume context
  - IntelBriefFull: company intel, contact background, talking points

Output (MatchResult):
  - match_score: float 0-100
    - 80-100: Strong fit (multiple skill overlaps, relevant experience, right seniority)
    - 60-79: Good fit (partial skill overlap, adjacent experience)
    - 40-59: Possible fit (domain match but skill gaps)
    - 0-39: Weak fit
  - value_proposition: 1-2 sentences specific to this company/role
    (MUST reference company name or talking point — generic statements are rejected)
  - confidence_level: "high" | "medium" | "low"
    - high: match_score >= 70
    - medium: 40 <= match_score < 70
    - low: match_score < 40

MATCH-02 scoring factors:
  - Skills overlap: exact matches between UserProfile.skills and company tech stack (~40%)
  - Experience relevance: UserProfile.experience domain alignment with company domain (~30%)
  - Seniority fit: experience depth vs. apparent company stage/team_size (~20%)
  - Company size fit: startup vs. enterprise preference signals (~10%)
"""
from dataclasses import dataclass
from datetime import datetime

from pydantic_ai import Agent, RunContext
from sqlalchemy.ext.asyncio import AsyncSession

from ingot.db.models import Lead, Match, LeadStatus
from ingot.models.schemas import UserProfile, IntelBriefFull, MatchResult


@dataclass
class MatcherDeps:
    user_profile: UserProfile       # Pydantic schema (from profile_agent output)
    intel_brief: IntelBriefFull     # Pydantic schema (from research_phase2 output)
    session: AsyncSession
    lead: Lead


matcher_agent = Agent(
    "anthropic:claude-3-5-haiku-latest",
    deps_type=MatcherDeps,
    output_type=MatchResult,
    system_prompt=(
        "You are a job search matching agent. Given a candidate's profile and a company's intel brief, "
        "produce a calibrated match score (0-100), a specific value proposition, and a confidence level. "
        "\n\n"
        "SCORING RUBRIC:\n"
        "  80-100: Strong fit — 3+ direct skill matches, directly relevant experience, right seniority\n"
        "  60-79: Good fit — 2 skill matches, adjacent experience, minor gaps\n"
        "  40-59: Possible fit — 1 skill match, domain alignment, clear gaps to address\n"
        "  0-39: Weak fit — few overlaps, significant domain or skill mismatch\n"
        "\n"
        "SCORING FACTORS (approximate weights):\n"
        "  - Skills overlap vs. company tech stack in description: ~40%\n"
        "  - Experience relevance to company's domain/product: ~30%\n"
        "  - Seniority fit (experience depth vs. company stage): ~20%\n"
        "  - Company size fit (startup vs. enterprise signals): ~10%\n"
        "\n"
        "VALUE PROPOSITION RULES:\n"
        "  - Must be 1-2 sentences maximum\n"
        "  - Must reference the specific company name OR a talking point\n"
        "  - Must mention a specific skill or experience from the UserProfile\n"
        "  - BAD: 'I am a strong fit for your engineering team'\n"
        "  - GOOD: 'My 3 years building payment APIs at Stripe maps directly to {company}'s "
        "infra challenges as a fintech scale-up'\n"
        "\n"
        "CONFIDENCE LEVEL:\n"
        "  - 'high' if match_score >= 70\n"
        "  - 'medium' if 40 <= match_score < 70\n"
        "  - 'low' if match_score < 40\n"
    ),
)


@matcher_agent.system_prompt
async def inject_profile_and_brief(ctx: RunContext[MatcherDeps]) -> str:
    """Inject UserProfile and IntelBriefFull into the system prompt context."""
    profile = ctx.deps.user_profile
    brief = ctx.deps.intel_brief

    return (
        f"\n\nCANDIDATE PROFILE:\n"
        f"Name: {profile.name}\n"
        f"Headline: {profile.headline}\n"
        f"Skills: {', '.join(profile.skills)}\n"
        f"Experience:\n" + "\n".join(f"  - {e}" for e in profile.experience) + "\n"
        f"Projects: {', '.join(profile.projects) if profile.projects else 'none'}\n"
        f"\nCOMPANY INTEL:\n"
        f"Company: {brief.company_name}\n"
        f"Product: {brief.company_product_description}\n"
        f"Signals: {'; '.join(brief.company_signals)}\n"
        f"Contact: {brief.person_name} — {brief.person_role}\n"
        f"Contact background: {brief.person_background}\n"
        f"Talking points:\n" + "\n".join(f"  {i+1}. {tp}" for i, tp in enumerate(brief.talking_points))
    )


async def run_matcher(deps: MatcherDeps) -> MatchResult:
    """
    Run the Matcher agent for a single Lead.

    Transitions Lead.status: approved -> matched
    Persists Match record to SQLite (MATCH-05).
    Returns MatchResult.

    MATCH-01, MATCH-02, MATCH-03, MATCH-04, MATCH-05
    """
    lead = deps.lead

    result = await matcher_agent.run(
        "Evaluate the match between this candidate and company. Return a calibrated MatchResult.",
        deps=deps,
    )
    match_result: MatchResult = result.output

    # Persist Match record (MATCH-05)
    match_record = Match(
        match_score=match_result.match_score,
        value_proposition=match_result.value_proposition,
        confidence_level=match_result.confidence_level,
        lead_id=lead.id,
        created_at=datetime.utcnow(),
    )
    deps.session.add(match_record)

    # Transition Lead status (MATCH-05: linked to IntelBrief and UserProfile)
    lead.status = LeadStatus.matched
    deps.session.add(lead)

    await deps.session.commit()
    await deps.session.refresh(match_record)

    return match_result
```
  </action>
  <verify>
    <automated>python -c "
from ingot.agents.matcher import MatcherDeps, matcher_agent, run_matcher
from ingot.models.schemas import MatchResult, UserProfile, IntelBriefFull
from pydantic import ValidationError

# Verify MatchResult schema enforces 0-100 range
try:
    MatchResult(match_score=150.0, value_proposition='test', confidence_level='high')
    print('ERROR: Should have rejected score > 100')
except ValidationError as e:
    print(f'Score range validation OK: {e.error_count()} error(s)')

# Verify score boundary at 0
try:
    MatchResult(match_score=-1.0, value_proposition='test', confidence_level='low')
    print('ERROR: Should have rejected negative score')
except ValidationError as e:
    print(f'Negative score validation OK')

# Verify valid MatchResult
mr = MatchResult(match_score=75.0, value_proposition='My Python experience aligns with Acme infra work', confidence_level='high')
assert mr.match_score == 75.0

# Verify matcher_agent has system_prompt injection registered
assert matcher_agent is not None

# Verify run_matcher is importable
import inspect
sig = inspect.signature(run_matcher)
assert 'deps' in sig.parameters

print('matcher.py imports and validation OK')
"
    </automated>
  </verify>
  <done>
    `MatcherDeps`, `matcher_agent`, and `run_matcher` all import from `ingot.agents.matcher`. `MatchResult` raises `ValidationError` for `match_score` outside 0-100 range. `matcher_agent` has `output_type=MatchResult`. `run_matcher()` takes a `MatcherDeps` argument. `Match` db record creation is wired with `lead_id` FK.
  </done>
</task>

</tasks>

<verification>
Run after task complete:

```bash
python -c "
from ingot.agents.matcher import MatcherDeps, matcher_agent, run_matcher
from ingot.models.schemas import MatchResult

# Full validation check
import inspect
assert inspect.iscoroutinefunction(run_matcher), 'run_matcher must be async'

# Confirm scoring rubric is in system_prompt
# (can't check runtime content without executing, but confirm agent is correctly configured)
print('Matcher agent configuration:')
print(f'  output_type: MatchResult')
print(f'  deps_type: MatcherDeps')
print(f'  run_matcher is async: {inspect.iscoroutinefunction(run_matcher)}')
print('  MatchResult score range 0-100: enforced by Pydantic ge/le validators')
print('Matcher verification OK')
"
```
</verification>

<success_criteria>
- `matcher_agent` uses `output_type=MatchResult`, `deps_type=MatcherDeps`
- `MatchResult.match_score` is validated 0-100 by Pydantic (`ge=0.0, le=100.0`)
- `run_matcher()` persists a `Match` record with `lead_id` FK and transitions `Lead.status` to "matched"
- `matcher_agent` system prompt includes the 4-factor scoring rubric and value proposition rules
- `MatcherDeps` injects `UserProfile` (Pydantic schema) and `IntelBriefFull` via `deps_type`
- MATCH-01 through MATCH-05 requirements all addressed
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-pipeline-scout-through-writer/02-04-SUMMARY.md` with:
- Model used for matcher_agent (haiku)
- MatchResult field names and validator rules
- Confidence level thresholds (high >= 70, medium 40-69, low < 40)
- Lead.status transitions: approved -> matched
- run_matcher() signature for Orchestrator (Plan 02-06) to reference
</output>
